{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "            .text_cell.rendered table, .text_cell.rendered td, .text_cell.rendered th {\n",
       "                font-size: 15px\n",
       "                color: #D05000;\n",
       "            }\n",
       "            div.warn {    \n",
       "                background-color: #fcf2f2;\n",
       "                border-color: #dFb5b4;\n",
       "                border-left: 5px solid #dfb5b4;\n",
       "                padding: 0.5em;\n",
       "            }\n",
       "            div.rmk {    \n",
       "                background-color: #d8ffe4;\n",
       "                border-color: #dFb5b4;\n",
       "                border-left: 5px solid #91aa99;\n",
       "                padding: 0.5em;\n",
       "            }\n",
       "            div.note {    \n",
       "                background-color: #cee9ff;\n",
       "                border-color: #dFb5b4;\n",
       "                border-left: 5px solid #86a2ba;\n",
       "                padding: 0.5em;\n",
       "            }\n",
       "            .rendered_html tr, .rendered_html th, .rendered_html td {\n",
       "                text-align: left;\n",
       "            }\n",
       "            .rendered_html :first-child {\n",
       "                text-align: left;\n",
       "            }\n",
       "            .rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "                font-size: 16px;\n",
       "            }\n",
       "            .rendered_html th, .rendered_html td {\n",
       "                border-bottom: 2px  #707070 solid;\n",
       "                padding: 12px;\n",
       "            }\n",
       "            .reveal table, .reveal td, .reveal th, .reveal tr {\n",
       "                font-size: 25px; }\n",
       "            .reveal il{\n",
       "                margin-bottom: -100px;\n",
       "            }\n",
       "            img[alt=drawing] { width: 200px; }\n",
       "            figure{\n",
       "                display: inline-block;\n",
       "            }\n",
       "        </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<style>\n",
    "            .text_cell.rendered table, .text_cell.rendered td, .text_cell.rendered th {\n",
    "                font-size: 15px\n",
    "                color: #D05000;\n",
    "            }\n",
    "            div.warn {    \n",
    "                background-color: #fcf2f2;\n",
    "                border-color: #dFb5b4;\n",
    "                border-left: 5px solid #dfb5b4;\n",
    "                padding: 0.5em;\n",
    "            }\n",
    "            div.rmk {    \n",
    "                background-color: #d8ffe4;\n",
    "                border-color: #dFb5b4;\n",
    "                border-left: 5px solid #91aa99;\n",
    "                padding: 0.5em;\n",
    "            }\n",
    "            div.note {    \n",
    "                background-color: #cee9ff;\n",
    "                border-color: #dFb5b4;\n",
    "                border-left: 5px solid #86a2ba;\n",
    "                padding: 0.5em;\n",
    "            }\n",
    "            .rendered_html tr, .rendered_html th, .rendered_html td {\n",
    "                text-align: left;\n",
    "            }\n",
    "            .rendered_html :first-child {\n",
    "                text-align: left;\n",
    "            }\n",
    "            .rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "                font-size: 16px;\n",
    "            }\n",
    "            .rendered_html th, .rendered_html td {\n",
    "                border-bottom: 2px  #707070 solid;\n",
    "                padding: 12px;\n",
    "            }\n",
    "            .reveal table, .reveal td, .reveal th, .reveal tr {\n",
    "                font-size: 25px; }\n",
    "            .reveal il{\n",
    "                margin-bottom: -100px;\n",
    "            }\n",
    "            img[alt=drawing] { width: 200px; }\n",
    "            figure{\n",
    "                display: inline-block;\n",
    "            }\n",
    "        </style>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch for Physicists <br><br> A Short Introduction to Tensor Computing and Artificial Neural Networks for Researchers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**WARNING**\n",
    "\n",
    "I will present first how the framework works to get an idea of how the computations are done. \n",
    "\n",
    "It is not necessary to remember the details!\n",
    "\n",
    "I will provide in the second part a way to write more quickly networks that work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "### Why deep learning frameworks?\n",
    "\n",
    "* Easily build **large models** (computational graphs)\n",
    "* Easy **gradient computation** (you do not want do do this by hand)\n",
    "* **Efficient of GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep learning frameworks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Python is arguably today the language of choice for deep learning.  \n",
    "PyTorch is one of the most popular ones. \n",
    "![title](img/trends.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some framework are **standalone libraries**, some are **built on an other framework** that works as a backend, usually providing a more user-friendly experience. \n",
    "\n",
    "We find in particular in this category [Keras](https://keras.io/) that sits on [TensorFlow](https://www.tensorflow.org/) (Google), Theano or CNTK (Microsoft) to provide an very simple and easy to use interface especially suitable for beginners.  \n",
    "\n",
    "Recently, [PyTorch Lightning](https://www.pytorchlightning.ai/) emerged to be the equivalent of Keras for PyTorch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So why PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### User-friendly\n",
    "\n",
    "* Easy for beginners (especially with PyTorch Lightning)\n",
    "* Large active community (as Keras/TensorFlow)\n",
    "* Made for research: easy to debug, customize and expand \n",
    "* Copies almost exactly **Numpy array syntax**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Imperative progamming\n",
    "\n",
    "**Imperative programming**: (PyTorch)  \n",
    "If you type something, it performs the calculations as you run the instructions (like Matlab/Python)\n",
    "\n",
    "\n",
    "**Declarative programming**: (Keras/TensorFlow)  \n",
    "You build a *graph*, that represents the calculation that you want to perform.\n",
    "Then is sent to the GPU, and only the final result is retrieved once the graph is compiled. \n",
    "\n",
    "*Imperative programming allows to test and debug the program as we would do with NumPy/Matlab.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast training\n",
    "\n",
    "PyTorch is *highly optimized for GPU calculation*\n",
    "\n",
    "with TensorFlow, one of the best framework for training speed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tensor calculation with PyTorch\n",
    "\n",
    "PyTorch can do basically **everything Matlab/NumPy does**, but on **GPUs** with the same syntax as **NumPy**  \n",
    "\n",
    "Interesting regardless of neural networks!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basic steps for deep learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Preparing the data\n",
    "\n",
    "One of the most important part!  \n",
    "There are many ways to present the data to a network, they usually yield different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Creating the model\n",
    "\n",
    "The model is the layout of the network.  \n",
    "Most network will only use the standard building blocks provided by PyTorch.  \n",
    "However, PyTorch allows to easily build custom layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Training the network\n",
    "\n",
    "Requires to:\n",
    "* Find a suitable cost function\n",
    "* Feed the a part of the dataset, *the training set*, to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Testing the network\n",
    "\n",
    "The rest of the dataset, *the testing set*, is used to test the *generaization* capabilities of the network.\n",
    "\n",
    "\n",
    "![title](img/earlystopping.png)\n",
    "Image from https://deeplearning4j.org/docs/latest/deeplearning4j-nn-early-stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Savig and using the model\n",
    "\n",
    "Obvious part..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Levels of abstractions in PyTorch\n",
    "\n",
    "3 levels:\n",
    "* **Tensors**, like NumPy, but can be stored/computed on GPU. Stores data and gradient (optional),\n",
    "* **Module**, a layer of the network. Stores the state of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Installing PyTorch\n",
    "\n",
    "**Conda will be already installed on the computation server, and compiled locally to be optimized.**\n",
    "\n",
    "But, to test on your computer, you can use the packaged version from conda.\n",
    "\n",
    "**CPU only:**  \n",
    "`conda install pytorch torchvision -c pytorch`\n",
    "\n",
    "**GPU support (Nvidia):**  \n",
    "`conda install pytorch torchvision cudatoolkit=10.2 -c pytorch`\n",
    "\n",
    "See https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using tensors: From NumPy to PyTorch\n",
    "\n",
    "* Very similar to NumPy but few differences.  \n",
    "The biggest one: arrays can be sent to the GPU!  \n",
    "\n",
    "\n",
    "* No built-in notion of deep learning, just arrays!  \n",
    "Can implement faster array type calculations even without deep learning in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First of, import PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create tensor from a list\n",
    "a = torch.tensor([1,2,3,4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a 10 by 5 random tensor with uniform distribution between 0 and 1\n",
    "b = torch.rand((10,5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a 10 by 5 by 3 random tensor with normal distribution of mean 0 and variance 1\n",
    "c = torch.randn((10,5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a tensor with all zero or one values\n",
    "d = torch.zeros((100,5))\n",
    "e = torch.ones((100,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From and to numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**from numpy array to torch tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "A = np.arange(9).reshape(3,3)\n",
    "B = torch.from_numpy(A.astype(np.float32))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**from torch to tensor numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]\n",
      " [6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "C = B.numpy()\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We did not specify which data type we wanted our tensors to be, so PyTorch chose for us.\n",
    "We can have access to the type of variable using dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4])\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.rand((10,5))\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Specify the data type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3,4],dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Casting (changing data type)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# equivalent to .astype() with numpy\n",
    "new_a = a.type(torch.float64)\n",
    "print(new_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Data types can be very important for deep learning applications as it sets a **limit on the precision** the calculation can reach.\n",
    "Moreover, **PyTorch can be restrictive compared to numpy** when it comes to operations with multiple tenors, usually requiring that they all are of the same type.\n",
    "\n",
    "More information about tensors data types can be found here:\n",
    "[pytorch.org/docs/stable/tensor_attributes.html](https://pytorch.org/docs/stable/tensor_attributes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Array's dimension and indexing\n",
    "\n",
    "Indexing is similarly to numpy arrays.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "# slicing from the second element to the penultimate element (not included) with a step of 2\n",
    "x[1:-1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(2,3,1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking the first element of the first dimension\n",
    "y = x[0,:,:]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can achieve the same result with\n",
    "y = x[0,...]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Increasing the dimension\n",
    "\n",
    "Often necessary to **add an singleton dimension**, meaning adding a dimension without adding data to the tensor, in order to match a given tensor size for operation.  \n",
    "\n",
    "For instance, you can come across a situation when you have 2D images when the program expect multi-channel images (like RGB images).  \n",
    "Then the software will expect a trhee dimension tensor, the last dimension being the channel dimension, which can be equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4).reshape(2,2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's add a singleton dimension in the last position\n",
    "y = x[...,None]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or in the middle position\n",
    "z = x[:,None,:]\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reducing the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(2,3,1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing the last singleton dimension\n",
    "y = x.squeeze(dim=-1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reshaping tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Often necessary to reshape a tensor without changing its number of elements.  \n",
    "\n",
    "**Example:** step between a convolutional layer, where data is represented by a 3D tensor, and a fully connected layer, where input data should be a 1D tensor.  \n",
    "\n",
    "There are many ways to manipulate tensors dimension but it is a good practice to use the `view()` function that guarantees that the  data is **not copied or modified internally**, only the metadata that tells python how to read the data. \n",
    "\n",
    "**Much faster** for large size tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(2,-1,1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(-1,6)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " </style><div class=note>\n",
    "**Note:**   The `-1`, it is a very useful way to tell the software to replace it by whatever number that allows conserving the total dimension of the tensor.\n",
    "Indeed, it is in this case equivalent to `x.view(1,6)` as `1*6 = 2*3*1`.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tensor merging and concatenation\n",
    "\n",
    "We can concatenate tensors along a given dimension so that this dimension will be the sum of the corresponding dimensions of the two arrays.  \n",
    "Other dimensions must be the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(2,3,1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate two tensors along the last dimension\n",
    "y = torch.cat((x,x), dim = -1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can also stack arrays in a way to create another dimension at the chosen position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(([[[1],[2],[3]], [[4],[5],[6]]]))\n",
    "# Stack two tensors along the second dimension\n",
    "y = torch.stack((x,x), dim = 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]])\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4],\n",
       "        [ 9, 16]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mul(x)\n",
    "# or equivalently\n",
    "x*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 10],\n",
       "        [15, 22]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.matmul(x)\n",
    "# or equivalently\n",
    "x @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Also works with batches of data (dim = 3 or 4)**\n",
    "\n",
    "Performs the operation on the last two dimensions for each 2d array of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((10,3,6))\n",
    "y = torch.rand((10,6,2))\n",
    "z = x@y\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Linear combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 10],\n",
       "        [13, 16]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]])\n",
    "y = torch.tensor([[5,6],[7,8]])\n",
    "z = 2*x+y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "tensor([0.5000, 2.5000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4,dtype = torch.float32).reshape(2,2)\n",
    "print(x)\n",
    "print(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.var(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Usual functions\n",
    "\n",
    "Usual functions such as exponential, sinus, log etc... are available with PyTorch tesnsors in a similar fashion as in numpy.  \n",
    "\n",
    "More information:\n",
    "* [PyTorch Tensor API](https://pytorch.org/docs/stable/tensors.html)\n",
    "* [PyTorch for NumPy Users](https://github.com/wkentaro/pytorch-for-numpy-users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Few useful ones different from numpy we will use in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`torch.clamp(x,min,max)`  \n",
    "$y_{i}=\\left\\{\\begin{array}{ll}{\\min } & {\\text { if } x_{i}<\\min } \\\\ {x_{i}} & {\\text { if } \\min \\leq x_{i} \\leq \\max } \\\\ {\\max } & {\\text { if } x_{i}>\\max }\\end{array}\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [2., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4,dtype = torch.float32).reshape(2,2)\n",
    "print(x)\n",
    "x.clamp(min = 2.)\n",
    "# or x = torch.clamp(x,min=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]])\n",
      "torch.Size([1, 1])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2]])\n",
    "print(x)\n",
    "# get the value of a tensor containing only one element\n",
    "print(x.shape)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False],\n",
       "       [False,  True]])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[3,4]])\n",
    "B = np.array([[1,1],[4,4]])\n",
    "np.equal(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1,2],[3,4]])\n",
    "B = torch.tensor([[1,1],[4,4]])\n",
    "torch.eq(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assigning tensors to CPU/GPU\n",
    "\n",
    "Unlike numpy, tensors can be assigned to a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the third GPU available\n",
    "#device = torch.device(\"cuda:2\")\n",
    "# or the CPU\n",
    "device = torch.device(\"cpu\")\n",
    "x = torch.tensor([1,2,3,4]).to(device)\n",
    "x.device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " </style><div class=rmk>\n",
    "**Remark:**  Subsequent computation of the tensor will be on the selected device.  \n",
    "    Calculations involving multiple tensors require those tensors to be on the same device.\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " </style><div class=note>\n",
    "**Note:**  It is at this step that you select the GPU you will work on. \n",
    "If there is multiple GPUs available on a shared computation server, it is worth checking the usage of the GPU before setting the GPU.\n",
    "<div/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done (usually in an SSH session on the server) using the command line `nvidia-smi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It returns the information about the memory, power consumption and usage of the GPUs.\n",
    "\n",
    "\n",
    "![](img/nvidiasmi.png)\n",
    "It is obviously a good idea to chose an unused GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " </style><div class=note>\n",
    "**Note 2:**  For deep learning applications, it will not be necessary to individually assign tensors to GPUs. \n",
    "Instead, we will later create a model that will contain all the tensors and parameters of our networks and assign the whole model to the GPU. \n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other ways to assign tensor to GPU/CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# upon creation\n",
    "A = torch.randn(3, 4, 5, device='cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# copy to CPU\n",
    "A.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#copy to GPU\n",
    "A.cuda(device = 'cuda:3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Testing calculation on GPU vs CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "let's connect on Fermat to do some test  \n",
    "`ssh spopoff@fermat`  \n",
    "`source activate py38`  \n",
    "`python file.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**NumPy on CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "d = 4000\n",
    "A = np.random.randn(d,d).astype(np.float32)\n",
    "B = np.random.randn(d,d).astype(np.float32)\n",
    "\n",
    "calc_function_CPU = lambda: np.dot(A,B)\n",
    "    \n",
    "    \n",
    "calc_time = timeit.timeit(calc_function_CPU, number = 10)\n",
    "print(f\"CPU took {calc_time}s to calculate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`CPU took 1.046984750777483s to calculate.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**PyTorch on GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import timeit\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "d = 4000\n",
    "\n",
    "A = torch.randn(d,d).to(device)\n",
    "B = torch.randn(d,d).to(device)\n",
    "\n",
    "calc_function_GPU = lambda: torch.mm(A,B)   \n",
    "    \n",
    "calc_time = timeit.timeit(calc_function_GPU, number = 10)\n",
    "print(f\"GPU took {calc_time}s to calculate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GPU took 0.014175284653902054s to calculate.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A small network with `tensors` only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](img/simple_fc.png)\n",
    "\n",
    "It is possible to write a neural network with Tensors only, but we have to do everything *manually*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "dtype  =  torch.float32\n",
    "\n",
    "# Defining input size, hidden layer size, output size and data size respectively\n",
    "n_in,n_h,n_out,data_size=5,50,3,64\n",
    "\n",
    "# create tensors for data and weights\n",
    "# initialize to random\n",
    "x = torch.randn(data_size,n_in).type(dtype)\n",
    "y = torch.randn(data_size,n_out).type(dtype)\n",
    "\n",
    "w1 = torch.randn(n_in,n_h).type(dtype)\n",
    "w2 = torch.randn(n_h,n_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(10):\n",
    "    # result of first layer\n",
    "    h = x@w1\n",
    "    # relu\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu@w2\n",
    "    # calculate loss (MSE)\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    \n",
    "    # need to calculate the gradients: backward propagation\n",
    "    grad_y_pred = 2.0*(y_pred-y)\n",
    "    grad_w2 = h_relu.t()@grad_y_pred\n",
    "    grad_h_relu = grad_y_pred@w2.t()\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t()@grad_h\n",
    "    \n",
    "    # update the weights (gradient descent)\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Obviously not the way to go..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autograd\n",
    "\n",
    "We can ask PyTorch to automatically compute and store gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### `requires_grad=True`\n",
    "\n",
    "**Tensors** have a property `requires_grad`. \n",
    "If set to `True`, it store both the value and the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6052], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "x.requires_grad = True\n",
    "\n",
    "#or\n",
    "x = torch.rand(1, requires_grad = True)\n",
    "\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`x.grad` stores the tensor of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Autograd\n",
    "\n",
    "Automatic calculation and storage of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9169])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 3*x**2\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`x.grad` is the gradient of `y` with respect to `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9169], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3448],\n",
       "        [0.4305]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((2, 1), requires_grad = True) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2268],\n",
       "        [2.8365]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x[0]**3 + 2*x[1]**2\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two element output for the gradient by the two variables `x[0]` and `x[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0659, 0.0533],\n",
       "        [0.2548, 0.6841]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((2, 2), requires_grad = True) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[-0.1057, -0.4682],\n",
      "        [ 0.4047,  1.3201]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "x = torch.Tensor([[0.1, 0.2, 0.3], [0.4, 0.6, 0.8]])\n",
    "\n",
    "W = torch.randn((2, 2), requires_grad=True)\n",
    "b = torch.randn((1, 3), requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "y = torch.abs(torch.sin(W@x+b))\n",
    "y_ref = torch.arange(6,dtype=torch.float32).reshape(2,3)\n",
    "print(y_ref.shape)\n",
    "loss = ((y-y_ref)**2).sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As long as we use `torch` functions, the gradient can be calculated automatically by *Autograd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To prevent tracking history and using memory, one can wrap a code block in with ` with torch.no_grad():`.   \n",
    "This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we donâ€™t need the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2, 2), requires_grad = True)\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our small network with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dtype  =  torch.float32\n",
    "\n",
    "# Defining input size, hidden layer size, output size and data size respectively\n",
    "n_in,n_h,n_out,data_size=5,50,3,64\n",
    "\n",
    "# create tensors for data and weights\n",
    "# initialize to random\n",
    "x = torch.randn(data_size,n_in).type(dtype)\n",
    "y = torch.randn(data_size,n_out).type(dtype)\n",
    "\n",
    "w1 = torch.randn(n_in,n_h).type(dtype)\n",
    "w2 = torch.randn(n_h,n_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(10):\n",
    "    # result of first layer\n",
    "    h = x@w1\n",
    "    # relu\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu@w2\n",
    "    # calculate loss (MSE)\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    \n",
    "    # need to calculate the gradients: backward propagation\n",
    "    grad_y_pred = 2.0*(y_pred-y)\n",
    "    grad_w2 = h_relu.t()@grad_y_pred\n",
    "    grad_h_relu = grad_y_pred@w2.t()\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t()@grad_h\n",
    "    \n",
    "    # update the weights (gradient descent)\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**After**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Defining input size, hidden layer size, output size and data size respectively\n",
    "n_in,n_h,n_out,data_size=5,50,3,64\n",
    "\n",
    "# create variables for data and weights + initialize to random\n",
    "\n",
    "x = torch.randn((data_size,n_in),requires_grad = False)\n",
    "y = torch.randn((data_size,n_out),requires_grad = False)\n",
    "\n",
    "w1 = torch.randn((n_in,n_h),requires_grad = True)\n",
    "w2 = torch.randn((n_h,n_out),requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(10):\n",
    "    # result of first layer\n",
    "    h = x@w1\n",
    "    # relu\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu@w2\n",
    "    # calculate loss (MSE)\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    \n",
    "    # backward propagation done by autograd\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights (gradient descent)\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Does the same thing as previously with `tensors` but the calculation of the gradients are done in the simple `loss.backward()` step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modules\n",
    "\n",
    "The third level of abstraction in PyTorch.  \n",
    "Modules represent layers, group of layers or full networks.  \n",
    "Provided building blocks are module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Structure\n",
    "\n",
    "A module is a `class` that has a:  \n",
    "* a `forward(x)` functions, that calculates the output for a given input `x`\n",
    "* a `backward()` functions, that calcualtes the gradients, allowing the backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A large variety of typical building blocks are already provided in PyTorch (we will see them later).  \n",
    "Usually, we will **never have to write `backward` functions**, as autograd will take care of it as long as the `forward` functions use `torch` functions or operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Usage\n",
    "\n",
    "Nothing much to do:\n",
    "* Initialize the instance of the module\n",
    "* use it *as a function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "x = torch.tensor([-1,0,1],dtype = torch.float32)\n",
    "\n",
    "fc = Linear(in_features = 3, out_features = 1)\n",
    "y = fc(x)\n",
    "\n",
    "y_ref = torch.tensor([1.],dtype = torch.float32)\n",
    "\n",
    "loss = (y-y_ref)**2\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our small network with `module`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Defining input size, hidden layer size, output size and data size respectively\n",
    "n_in,n_h,n_out,data_size=5,50,3,64\n",
    "\n",
    "# create variables for data and weights + initialize to random\n",
    "\n",
    "x = torch.randn((data_size,n_in),requires_grad = False)\n",
    "y = torch.randn((data_size,n_out),requires_grad = False)\n",
    "\n",
    "w1 = torch.randn((n_in,n_h),requires_grad = True)\n",
    "w2 = torch.randn((n_h,n_out),requires_grad = True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(10):\n",
    "    # result of first layer\n",
    "    h = x@w1\n",
    "    # relu\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu@w2\n",
    "    # calculate loss (MSE)\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    \n",
    "    # backward propagation done by autograd\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights (gradient descent)\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**After**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Defining input size, hidden layer size, output size and data size respectively\n",
    "n_in,n_h,n_out,data_size=5,50,3,64\n",
    "\n",
    "# create variables for data and weights + initialize to random\n",
    "x = torch.randn((data_size,n_in),requires_grad = False)\n",
    "y = torch.randn((data_size,n_out),requires_grad = False)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_in,n_h),   # dense layer, i.e. matrix multiplication\n",
    "            torch.nn.ReLU(),             # relu, non-linear activation function\n",
    "            torch.nn.Linear(n_h,n_out))  # second dense layer\n",
    "# create loss function\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) # Mean square error\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(10):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred,y)\n",
    "    # backward propagation done by autograd\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameters\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate*param.grad.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_in,n_h),   # dense layer, i.e. matrix multiplication\n",
    "            torch.nn.ReLU(),             # relu, non-linear activation function\n",
    "            torch.nn.Linear(n_h,n_out))  # second dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "is equivalent to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class my_net(torch.nn.module):\n",
    "    def __init__(self):\n",
    "        # create the modules/layers we will use\n",
    "        self.fc1  = torch.nn.Linear(n_in,n_h)   # dense layer, i.e. matrix multiplication\n",
    "        self.relu = torch.nn.ReLU()             # relu, non-linear activation function\n",
    "        self.fc2  = torch.nn.Linear(n_h,n_out)  # second dense layer\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "</style><div class=note>\n",
    "**Note:** \n",
    "   Again, no `backward` function as it is calculated automatically by autograd!\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimizers\n",
    "\n",
    "They update the weights for you!  \n",
    "We will see the standard *stochastic gradient descent*, we will show more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Create the optimizer object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Need to set the grad to 0** (otherwise they accumulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Update the parameters using the parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our small network with `optimizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Before**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/py38/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Defining input size, hidden layer size, output size and data size respectively\n",
    "n_in,n_h,n_out,data_size=5,50,3,64\n",
    "\n",
    "# create variables for data and weights + initialize to random\n",
    "x = torch.randn((data_size,n_in),requires_grad = False)\n",
    "y = torch.randn((data_size,n_out),requires_grad = False)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_in,n_h),   # dense layer, i.e. matrix multiplication\n",
    "            torch.nn.ReLU(),             # relu, non-linear activation function\n",
    "            torch.nn.Linear(n_h,n_out))  # second dense layer\n",
    "\n",
    "# create loss function\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) # Mean square error\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(10):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    \n",
    "    # backward propagation done by autograd\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameters\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate*param.grad.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**After**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Defining input size, hidden layer size, output size and data size respectively\n",
    "n_in,n_h,n_out,data_size=5,50,3,64\n",
    "\n",
    "# create variables for data and weights + initialize to random\n",
    "x = torch.randn((data_size,n_in),requires_grad = False)\n",
    "y = torch.randn((data_size,n_out),requires_grad = False)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_in,n_h),   # dense layer, i.e. matrix multiplication\n",
    "            torch.nn.ReLU(),             # relu, non-linear activation function\n",
    "            torch.nn.Linear(n_h,n_out))  # second dense layer\n",
    "\n",
    "# create loss function\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) # Mean square error\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "for epoch in range(10):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    \n",
    "    optimizer.zero_grad() # before the backpropagation!   \n",
    "    # backward propagation done by autograd\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "We do not know how to create a full neural network yet, by we know how to optimize a model.\n",
    "We will try to solve a problem that consists in optimizing a simple model using an optimizer and the autograd.\n",
    "\n",
    "Here is the physical problem:\n",
    "\n",
    "* Let's consider a linear system characterized by a transmission matrix $\\mathbf{H}$ (real) of size `n_out` $\\times$ `n_in`.\n",
    "* We send a collection of `n_inputs` random inputs, reprenseted by a matrix $\\mathbf{X}$ of size `n_inputs` $\\times$ `n_in`.\n",
    "* We retrive the output corresponding to $\\mathbf{Y} = \\mathbf{X} \\times \\mathbf{H}^t$ of size `n_inputs` $\\times$ `n_out`.\n",
    "* We add a noise with a variance `noise_var` to $\\mathbf{Y}$.\n",
    "\n",
    "Because we know the system is fully described by a matrix $\\mathbf{H}$, we will use a **network** with only one layer corresponding to a **dense layer** (i.e. one matrix multiplication, the coefficients being the parameters to be trained. \n",
    "\n",
    "Using the above example, create and train the system to find the correct matrix that correctly predict $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## PARAMETERS\n",
    "\n",
    "# size of the system matrix\n",
    "n_in, n_out = 6,10\n",
    "# number of random input we use to estimate the response of the system\n",
    "n_inputs = 25\n",
    "# noise level (relative to the signal)\n",
    "noise_var = 5e-2\n",
    "\n",
    "## GENERATE THE DATA FOR THE SYSTEM IN NUMPY\n",
    "\n",
    "# transmission matrix\n",
    "H = np.random.randn(n_out, n_in)\n",
    "\n",
    "# inputs\n",
    "X = np.random.randn(n_inputs, n_in)\n",
    "\n",
    "# outputs\n",
    "Y = X@H.transpose() \n",
    "\n",
    "# noise\n",
    "noise = np.random.randn(*Y.shape)\n",
    "noise *= np.linalg.norm(Y)/np.linalg.norm(noise)*noise_var\n",
    "\n",
    "Y += noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CONVERT DATA TO PYTORCH AND COPY TO GPU (IF AVAILABLE)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_pt = torch.from_numpy(X).to(device).type(torch.float)\n",
    "Y_pt = torch.from_numpy(Y).to(device).type(torch.float)\n",
    "H_pt = torch.from_numpy(H).to(device).type(torch.float)\n",
    "\n",
    "## MODEL \n",
    "model = torch.nn.Linear(n_in, n_out).to(device)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A quick example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Before looking under the hood, let's see how to write the simpler network we can imagine, a fully connected network with `n_in` inputs and `n_out` outputs and one hidden layer with 10 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](img/simple_fc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1: Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Defining input size, hidden layer size, output size respectively\n",
    "n_in,n_h,n_out=5,50,3\n",
    "# training and testing size\n",
    "n_train, n_test = 10000, 1000\n",
    "# noise level\n",
    "noise = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: Generate the data\n",
    "\n",
    "We want the network to be able to differenciate 5 different binary patterns in presence of noise.  \n",
    "The input of the network will be the signals and the outputs the labels (the index of the pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element looks like: tensor([ 0.9342,  0.7971, -0.2005,  0.2592,  0.3444])\n",
      "corresponding to patter #0\n"
     ]
    }
   ],
   "source": [
    "device  = torch.device(\"cpu\")#torch.device(\"cuda:1\")\n",
    "patterns = torch.tensor([[1,1,0,0,0],\n",
    "                         [0,0,1,1,1],\n",
    "                         [1,0,1,0,1]],\n",
    "                        device = device, dtype = torch.float32)\n",
    "\n",
    "# generate a list of integer between 0 and 5 as input information\n",
    "labels_train = torch.randint(0,n_out, (n_train,))\n",
    "# generate corresponding signal and add gaussian noise\n",
    "signals_train = patterns[labels_train]+torch.randn(n_train,n_in)*noise\n",
    "\n",
    "print(f\"The first element looks like: {signals_train[0]}\\ncorresponding to patter #{labels_train[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: Create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model=nn.Sequential(nn.Linear(n_in,n_h),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_h,n_out),\n",
    "                    nn.LogSoftmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Step 4: Define the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "criterion=torch.nn.NLLLoss()\n",
    "# Construct the optimizer (Stochastic Gradient Descent)\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=.05,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 5: Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t loss: 1.128e+00\t correct (train) = 33.4%\n",
      "epoch: 1\t loss: 1.107e+00\t correct (train) = 33.6%\n",
      "epoch: 2\t loss: 1.069e+00\t correct (train) = 36.3%\n",
      "epoch: 3\t loss: 1.020e+00\t correct (train) = 77.7%\n",
      "epoch: 4\t loss: 9.619e-01\t correct (train) = 97.5%\n",
      "epoch: 5\t loss: 8.992e-01\t correct (train) = 97.5%\n",
      "epoch: 6\t loss: 8.330e-01\t correct (train) = 97.7%\n",
      "epoch: 7\t loss: 7.639e-01\t correct (train) = 98.2%\n",
      "epoch: 8\t loss: 6.919e-01\t correct (train) = 98.8%\n",
      "epoch: 9\t loss: 6.174e-01\t correct (train) = 99.5%\n",
      "epoch: 10\t loss: 5.413e-01\t correct (train) = 99.8%\n",
      "epoch: 11\t loss: 4.660e-01\t correct (train) = 99.9%\n",
      "epoch: 12\t loss: 3.941e-01\t correct (train) = 100.0%\n",
      "epoch: 13\t loss: 3.287e-01\t correct (train) = 100.0%\n",
      "epoch: 14\t loss: 2.718e-01\t correct (train) = 100.0%\n",
      "epoch: 15\t loss: 2.240e-01\t correct (train) = 100.0%\n",
      "epoch: 16\t loss: 1.846e-01\t correct (train) = 100.0%\n",
      "epoch: 17\t loss: 1.524e-01\t correct (train) = 100.0%\n",
      "epoch: 18\t loss: 1.261e-01\t correct (train) = 100.0%\n",
      "epoch: 19\t loss: 1.045e-01\t correct (train) = 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing the inputs to the model\n",
    "    output_train=model(signals_train) # Outputs are a list of \"probabilities\" for each possible pattern\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss_train=criterion(output_train,labels_train)\n",
    "    \n",
    "    # Get the index of max probability, i.e. the pattern that the model guessed\n",
    "    labels_pred_train = output_train.argmax(dim=1, keepdim=True) \n",
    "    # number of correct guesses\n",
    "    correct_train = labels_pred_train.eq(labels_train.view_as(labels_pred_train)).sum().item()\n",
    "    # percentage of success\n",
    "    sucess_rate_train = correct_train/n_train*100\n",
    "    \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    # perform a backward pass (backpropagation), i.e. it calculates all the gradients for us\n",
    "    loss_train.backward()\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch: {}\\t loss: {:.3e}\\t correct (train) = {:3.1f}%\".format(epoch,loss_train.item(), sucess_rate_train))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Easy improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Testing set\n",
    "\n",
    "The accuracy we have is obtained on the training set, so on data the algorithm has already seen.  \n",
    "As we saw previously, there is chances of overfitting, we need to test on data that the network has not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = torch.randint(0,n_out, (n_test,))\n",
    "signals_test = patterns[labels_test]+torch.randn(n_test,n_in)*noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We do the same thing as for the train set, **but** we do not update the weights using `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_test=model(signals_test)\n",
    "    loss_test=criterion(output_test,labels_test)\n",
    "    labels_pred_test = output_test.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "    correct_test = labels_pred_test.eq(labels_test.view_as(labels_pred_test)).sum().item()\n",
    "    sucess_rate_test = correct_test/n_test*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#reset the paramters\n",
    "model=nn.Sequential(nn.Linear(n_in,n_h),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_h,n_out),\n",
    "                    nn.LogSoftmax()).to(device) ## Model on the selected device!\n",
    "criterion=torch.nn.NLLLoss()\n",
    "# Construct the optimizer (Stochastic Gradient Descent)\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=.05,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t loss: 1.120e+00\t correct (train) = 8.4\t correct (test) = 28.5%\n",
      "epoch: 1\t loss: 1.094e+00\t correct (train) = 29.2\t correct (test) = 63.4%\n",
      "epoch: 2\t loss: 1.046e+00\t correct (train) = 62.8\t correct (test) = 80.8%\n",
      "epoch: 3\t loss: 9.824e-01\t correct (train) = 81.4\t correct (test) = 92.9%\n",
      "epoch: 4\t loss: 9.088e-01\t correct (train) = 92.5\t correct (test) = 98.0%\n",
      "epoch: 5\t loss: 8.292e-01\t correct (train) = 97.5\t correct (test) = 99.4%\n",
      "epoch: 6\t loss: 7.465e-01\t correct (train) = 99.4\t correct (test) = 99.6%\n",
      "epoch: 7\t loss: 6.630e-01\t correct (train) = 99.8\t correct (test) = 99.8%\n",
      "epoch: 8\t loss: 5.810e-01\t correct (train) = 99.9\t correct (test) = 100.0%\n",
      "epoch: 9\t loss: 5.028e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 10\t loss: 4.304e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 11\t loss: 3.652e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 12\t loss: 3.078e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 13\t loss: 2.581e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 14\t loss: 2.158e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 15\t loss: 1.800e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 16\t loss: 1.501e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 17\t loss: 1.254e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 18\t loss: 1.051e-01\t correct (train) = 100.0\t correct (test) = 100.0%\n",
      "epoch: 19\t loss: 8.851e-02\t correct (train) = 100.0\t correct (test) = 100.0%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    # tell the module we will be training (some layers have different behaivors for train and test)\n",
    "    model.train()\n",
    "    output_train=model(signals_train) \n",
    "    loss_train=criterion(output_train,labels_train)\n",
    "    labels_pred_train = output_train.argmax(dim=1, keepdim=True) \n",
    "    correct_train = labels_pred_train.eq(labels_train.view_as(labels_pred_train)).sum().item()\n",
    "    sucess_rate_train = correct_train/n_train*100\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval() # tell the module we are not training\n",
    "    with torch.no_grad():\n",
    "        output_test=model(signals_test)\n",
    "        loss_test=criterion(output_test,labels_test)\n",
    "        labels_pred_test = output_test.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        correct_test = labels_pred_test.eq(labels_test.view_as(labels_pred_test)).sum().item()\n",
    "        sucess_rate_test = correct_test/n_test*100\n",
    "\n",
    "    print(\"epoch: {}\\t loss: {:.3e}\\t correct (train) = {:3.1f}\\t correct (test) = {:3.1f}%\".format(\n",
    "                                                epoch,loss_train.item(),\n",
    "                                                sucess_rate_train, sucess_rate_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " </style><div class=note>\n",
    "**Note:**  Now we use both for training and inference (testing), we need to tell the model when we are in each situation using `model.train()` and `model.eval()`.  \n",
    "Some layers, such as `batchnorm`, have a different behavior during inference and training, and forgetting to use `model.eval()` will cause the system to continue to change when calculating on the testing set.  \n",
    "We then need to set `model.train()` during the training part to switch back to training.\n",
    "<div/>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataLoaders\n",
    "\n",
    "The previous example works fine as the whole dataset fits easily into our GPU memory.  \n",
    "That would not be the case for large datasets. \n",
    "\n",
    "We usually use *Mini-batch gradient descent* optmizers, or similar approaches, that requires to take use a *part* of the data set to update the parameters. \n",
    "\n",
    "We will use a `DataLoader`, that allows sampling a `Dataset` to train on part of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# gather both inputs (signals) and outputs (labels) in datasets\n",
    "train_dataset = TensorDataset(signals_train,labels_train)\n",
    "test_dataset = TensorDataset(signals_test,labels_test)\n",
    "\n",
    "# create dataloaders, it creates iterators that will return a number of elements corresponding to the batch size given\n",
    "batch_size = 64\n",
    "test_batch_size = n_test\n",
    "test_loader = DataLoader(test_dataset,batch_size=test_batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each epochs, instead of training on the whole data set in one time, we will iterate on small packages, or *batches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tLoss: 1.383842\tCorrect: 9999/10000\n",
      "Train Epoch: 1\tLoss: 0.481260\tCorrect: 9999/10000\n",
      "Train Epoch: 2\tLoss: 0.348202\tCorrect: 9999/10000\n",
      "Train Epoch: 3\tLoss: 0.287367\tCorrect: 9999/10000\n",
      "Train Epoch: 4\tLoss: 0.243659\tCorrect: 10000/10000\n",
      "Train Epoch: 5\tLoss: 0.212670\tCorrect: 9999/10000\n",
      "Train Epoch: 6\tLoss: 0.191116\tCorrect: 9999/10000\n",
      "Train Epoch: 7\tLoss: 0.178815\tCorrect: 9999/10000\n",
      "Train Epoch: 8\tLoss: 0.165254\tCorrect: 9999/10000\n",
      "Train Epoch: 9\tLoss: 0.156728\tCorrect: 9999/10000\n",
      "Train Epoch: 10\tLoss: 0.144865\tCorrect: 9999/10000\n",
      "Train Epoch: 11\tLoss: 0.135729\tCorrect: 9999/10000\n",
      "Train Epoch: 12\tLoss: 0.133588\tCorrect: 9998/10000\n",
      "Train Epoch: 13\tLoss: 0.126325\tCorrect: 9999/10000\n",
      "Train Epoch: 14\tLoss: 0.134224\tCorrect: 9999/10000\n"
     ]
    }
   ],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    # tell the module we will be training (some layers have different behaivors for train and test)\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch_idx, (signals, labels) in enumerate(train_loader):\n",
    "        # put the data on the device, previously on the CPU\n",
    "        data, target = signals.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # calculate the prediction of the model\n",
    "        output = model(signals)\n",
    "        labels_pred = output.argmax(dim=1, keepdim=True) \n",
    "        # estimate the loss\n",
    "        loss = criterion(output,labels)\n",
    "        # backward propagation and update of the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the global loss and correct\n",
    "        running_loss += loss.item()\n",
    "        total_correct += labels_pred.eq(labels.view_as(labels_pred)).sum().item()\n",
    "        \n",
    "    print('Train Epoch: {}\\tLoss: {:.6f}\\tCorrect: {}/{}'.format(\n",
    "                epoch, running_loss,\n",
    "                total_correct, len(train_loader.dataset)\n",
    "                ))\n",
    "    \n",
    "for epoch in range(15):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Same thing to do with `test()`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Terminology for training\n",
    "\n",
    "* An **epoch** is one forward and one backward pass for *all* the training example\n",
    "* The **batch size** is the number of of training example in one forward/backward pass to update the weight. This is the actual data that is sent onto the GPU, so larger batches mean filling more GPU memory.\n",
    "* An **iteration** is the treatment of one batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building blocks\n",
    "\n",
    "`torch.nn` doc [here](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "We present here only the most common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Functions\n",
    "\n",
    "\n",
    "Layers with no trainable parameters, such as *ReLU*, can be used without using the module, look at those two examples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear, ReLU\n",
    "model=nn.Sequential(nn.Linear(n_in,n_h),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_h,n_out),\n",
    "                    nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " </style><div class=warn>\n",
    "**Warning:** You have to use the module to use `Sequential`\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "class my_net(Module):\n",
    "    def __init__(self, n_in,n_h,n_out):\n",
    "        super().__init__()\n",
    "        self.fc1  = torch.nn.Linear(n_in,n_h)   # dense layer, i.e. matrix multiplication\n",
    "        self.fc2  = torch.nn.Linear(n_h,n_out)  # second dense layer\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return relu(x)\n",
    "    \n",
    "model = my_net(n_in = 10, n_h = 30, n_out = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can put almost whatever you want in the forward as long at it uses torch functions/modules so that the gradient can be calculated autmoatically**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
    "\n",
    "Parameters:  \n",
    "* `n_features` â€“ size of each input sample\n",
    "* `out_features` â€“ size of each output sample\n",
    "* `bias` â€“ If set to False, the layer will not learn an additive bias ($b$). Default: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`\n",
    "\n",
    "* `stride` controls the stride for the cross-correlation, a single number or a one-element tuple.\n",
    "* `padding` controls the amount of implicit zero-paddings on both sides for padding number of points.\n",
    "* `dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.\n",
    "* `groups` controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example, at groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='img/no_padding_no_strides_transposed.gif' alt='missing' width=\"200\"/>\n",
    "    <figcaption>no padding, no strides</figcaption>\n",
    "</figure>&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "<figure>\n",
    "    <img src='img/arbitrary_padding_no_strides.gif' alt='missing' width=\"200\"/>\n",
    "    <figcaption>padding, no strides</figcaption>\n",
    "</figure>&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "<figure>\n",
    "    <img src='img/no_padding_strides.gif' alt='missing' width=\"200\"/>\n",
    "    <figcaption>strides, no padding</figcaption>\n",
    "</figure>&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "<figure>\n",
    "    <img src='img/dilation.gif' alt='missing' width=\"200\"/>\n",
    "    <figcaption>dilation</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar behavior for 1D and 3D connvolutions:\n",
    "\n",
    "`torch.nn.Conv1d`  \n",
    "`torch.nn.Conv3d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pooling layers\n",
    "\n",
    "Allow to reduce the dimension of data, usually after convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Max Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`\n",
    "\n",
    "Take the maximum squares of size `kernel_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similar behavior for 1D and 3D maxpool:\n",
    "  \n",
    "`torch.nn.MaxPool1d`\n",
    "`torch.nn.MaxPool3d`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)`\n",
    "Calculat the average over  squares of size `kernel_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### ReLU\n",
    "\n",
    "\n",
    "**module:**\n",
    "`torch.nn.ReLU(inplace=False)`\n",
    "\n",
    "**function:**  \n",
    "`torch.nn.functional.relu(input, inplace=self.inplace)`\n",
    "\n",
    "Applies the rectified linear unit function element-wise:  \n",
    "$\\text{ReLU}(x)= \\max(0, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " </style><div class=warn>\n",
    "**Note:** \n",
    "* Many variants, also `leaky relu`, `ELU`, `sigmoid`, `tanh`...\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Softmax\n",
    "\n",
    "\n",
    "**module:**  \n",
    "`torch.nn.Softmax(dim=None)`\n",
    "\n",
    "**funciton:**  \n",
    "`torch.nn.functional.softmax(input, self.dim, _stacklevel=5)`\n",
    "\n",
    "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.\n",
    "\n",
    "Softmax is defined as:  \n",
    "$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " </style><div class=warn>\n",
    "**Note:** \n",
    "* The output is normalized to 1 and is positive, it usefull to output probabilities.\n",
    "* usually used as last layer for classification. \n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Log Softmax\n",
    "\n",
    "**module:**  \n",
    "`torch.nn.LogSoftmax(dim=None)`\n",
    "\n",
    "**function:**\n",
    "`torch.nn.functional.log_softmax(input, self.dim, _stacklevel=5)`\n",
    "\n",
    "Applies the $\\log(\\text{Softmax}(x))$ function to an n-dimensional input Tensor.  \n",
    "The LogSoftmax formulation can be simplified as:\n",
    "\n",
    "\n",
    "$\\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Normalization layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##### Batch Normalization\n",
    "\n",
    "\n",
    "`torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)`\n",
    "\n",
    "\n",
    "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: \n",
    "\n",
    "$\\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "**Why?**\n",
    "\n",
    "The distribition of the data changes inside the intermediate layers, making it more difficult for the network to adapt itself.  \n",
    "Batch normalization allows a better uniformity that can help to significanlty increase the convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Drop out\n",
    "\n",
    "`torch.nn.Dropout(p=0.5, inplace=False)`\n",
    "\n",
    "`torch.nn.Dropout2d(p=0.5, inplace=False)`\n",
    "\n",
    "`torch.nn.Dropout3d(p=0.5, inplace=False)`\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n",
    "\n",
    "**Why?**  \n",
    "To reduce overfitting, forbidding the system to hold most of the information on few parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss_function = torch.nn.XXLoss(param)\n",
    "loss = loss_function(estiamtion,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### L1 loss\n",
    "\n",
    "`torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')`\n",
    "\n",
    "Creates a criterion that measures the mean absolute error (MAE) between each element in the input $x$ and target $y$.\n",
    "\n",
    "The unreduced (i.e. with `reduction` set to `'none'`) loss can be described as:  \n",
    "\n",
    "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left| x_n - y_n \\right|$\n",
    "\n",
    "where $N$ is the batch size. If `reduction` is not `'none'` (default 'mean'), then:  \n",
    "\n",
    "$\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), \\quad \\text{if reduction} = \\text{'mean'}\\\\ \\operatorname{sum}(L), \\quad \\text{if reduction} = \\text{'sum'} \\end{cases}$\n",
    "\n",
    "$x$ and $y$ are tensors of arbitrary shapes with a total of nnn elements each.\n",
    "\n",
    "The sum operation still operates over all the elements, and divides by $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### MSE loss (L2 loss)\n",
    "\n",
    "`torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`\n",
    "\n",
    "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xxx and target $y$.\n",
    "\n",
    "The unreduced (i.e. with `reduction` set to `'none'`) loss can be described as:  \n",
    "\n",
    "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left( x_n - y_n \\right)^2$\n",
    "\n",
    "where $N$ is the batch size. If `reduction` is not `'none'` (default 'mean'), then:  \n",
    "\n",
    "$\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), \\quad \\text{if reduction} = \\text{'mean'}\\\\\n",
    "\\operatorname{sum}(L), \\quad \\text{if reduction} = \\text{'sum'} \\end{cases}$\n",
    "\n",
    "$x$ and $y$ are tensors of arbitrary shapes with a total of $n$ elements each.\n",
    "\n",
    "The sum operation still operates over all the elements, and divides by $n$.\n",
    "\n",
    "The division by $n$ can be avoided if one sets `reduction` = `'sum'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Negative log likelyhood loss\n",
    "\n",
    "`torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')`\n",
    "\n",
    "\n",
    "The negative log likelihood loss. It is useful to train a **classification** problem with C classes (typically with `nn.LogSoftmax()`).\n",
    "\n",
    "The `input` given through a forward call is expected to contain log-probabilities of each class. `input` has to be a Tensor of size either $(minibatch, C)$ or $(minibatch, C, d_1, d_2, ..., d_K)$ $\\geq 1$ for the $K$-dimensional case.\n",
    "\n",
    "The unreduced (i.e. with reduction set to 'none') loss can be described as:  \n",
    "\n",
    "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_{y_n} x_{n,y_n}, \\quad w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\}$\n",
    "\n",
    "For `weights` = `'none'`:\n",
    "\n",
    "$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - x_{n,y_n}$\n",
    "\n",
    "$y$ is the index of the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![](img/neg_log_demo.png)\n",
    "    Image from [ljvmiranda921.github.io/](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy loss\n",
    "\n",
    "`torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')`\n",
    "\n",
    "This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizers\n",
    "\n",
    "PyTorch API documentation [here](https://pytorch.org/docs/stable/optim.html).\n",
    "Very nice tutorial on optimizers [here](http://ruder.io/optimizing-gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Quick reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Batch Gradient descent**\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "$\\eta$: learning rate\n",
    "\n",
    "Idea: \n",
    "* Compute the slope (gradient) of the loss function at the current point\n",
    "* Move in the opposite direction of the slope increase by the computed amount\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/GD.png\" width=\"600\">    \n",
    "</center>\n",
    "\n",
    "![test](Pictures/GD.png)\n",
    "\n",
    "\n",
    "Gradient calculated on the all batch.\n",
    "\n",
    "* Memmory demending\n",
    "* Can be trapped in local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Stochastic gradient descent**\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i)} ; y^{(i)}\\right)$ \n",
    "\n",
    "Parameter update for each training example\n",
    "\n",
    "**Pros**\n",
    "* Frequent updates of parameters\n",
    "* Skip local minima\n",
    "\n",
    "**Cons**\n",
    "* Noisy anf slow convergence\n",
    "* Computationally expensive\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/stochastic-vs-batch-gradient-descent.png\" width=\"600\"> \n",
    "    <caption>Image from [bogotobogo.com/](https://www.bogotobogo.com/python/scikit-learn/scikit-learn_batch-gradient-descent-versus-stochastic-gradient-descent.php)</caption>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Mini-batch gradient descent**\n",
    "\n",
    "The *trade-off*\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i : i+n)} ; y^{(i : i+n)}\\right)$\n",
    "\n",
    "Parameters update for mini-batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Importance of learning rage**\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "$\\eta$: learning rate\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/lr.png\" width=\"600\"> \n",
    "    <caption>Image from [blog.algorithmia.com](https://blog.algorithmia.com/introduction-to-optimizers/)</caption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Momentum**\n",
    "\n",
    "$\\nu_j=m \\cdot \\nu_{j-1} + (1-m)\\nabla_{\\theta}\\cdot J(\\theta_{j-1}) \\\\\n",
    "\\theta_j=\\theta_{j-1}-\\eta \\cdot \\nu_j$\n",
    "\n",
    "$m \\in [0,1]$: momentum\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/momentum.png\" width=\"600\"> \n",
    "    <caption>Images from [medium.com](https://medium.com/machine-learning-bites/deeplearning-series-deep-neural-networks-tuning-and-optimization-39250ff7786d)</caption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Popular variants**\n",
    "\n",
    "* **RMSprop**\n",
    "\n",
    "RMSProp choses a different learning rate for each parameter and adapts the learning rate.\n",
    "\n",
    "* **Adam**\n",
    "\n",
    "Combines the heuristics of both Momentum and RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input), target).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stochastig gradient descent\n",
    "\n",
    "`torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)`\n",
    "\n",
    "\n",
    "Implements stochastic gradient descent (optionally with momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSprop\n",
    "\n",
    "`torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam\n",
    "`torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolutional image classifier\n",
    "\n",
    "Taken form [github/pytorch/examples](https://github.com/pytorch/examples/tree/master/mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "device  = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Import MNIST dataset\n",
    "\n",
    "\n",
    "![](img/mnist.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(\"./\", train=True, \n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                             target_transform=None, \n",
    "                             download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, \n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                             target_transform=None, \n",
    "                             download=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><figure>\n",
    "    <img src='img/classifier.svg' alt='missing'/>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 20, \n",
    "                               kernel_size = 5, stride = 1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 20, out_channels = 50, \n",
    "                               kernel_size = 5, stride = 1)\n",
    "        self.fc1 = nn.Linear(in_features = 4*4*50, out_features = 500)\n",
    "        self.fc2 = nn.Linear(in_features = 500, out_features = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        # 2D to 1D vector\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 350 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('>> Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.322089\n",
    "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.413550\n",
    "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.251130\n",
    ">> Test set: Average loss: 0.1933, Accuracy: 56480/60000 (94.1%)\n",
    "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.276879\n",
    "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.122182\n",
    "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.042489\n",
    ">> Test set: Average loss: 0.1057, Accuracy: 58164/60000 (96.9%)\n",
    ">> Test set: Average loss: 0.0836, Accuracy: 58466/60000 (97.4%)\n",
    ">> Test set: Average loss: 0.0646, Accuracy: 58814/60000 (98.0%)\n",
    ">> Test set: Average loss: 0.0532, Accuracy: 59039/60000 (98.4%)\n",
    ">> Test set: Average loss: 0.0486, Accuracy: 59094/60000 (98.5%)\n",
    ">> Test set: Average loss: 0.0412, Accuracy: 59232/60000 (98.7%)\n",
    ">> Test set: Average loss: 0.0377, Accuracy: 59296/60000 (98.8%)\n",
    ">> Test set: Average loss: 0.0358, Accuracy: 59319/60000 (98.9%)\n",
    ">> Test set: Average loss: 0.0306, Accuracy: 59435/60000 (99.1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: Fully connected classifier\n",
    "\n",
    "**Goal:** Do the same classifier as before but with fully connected layers.\n",
    "\n",
    "Everything the same, just change the model.\n",
    "\n",
    "Input images are `28` by `28`.\n",
    "\n",
    "<center><figure>\n",
    "    <img src='img/dense_net.svg' alt='missing'/>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = DenseNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Denoising autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device  = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs = 250\n",
    "batch_size =100\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Download the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Download Data\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\"./\", train=True, \n",
    "                                     transform=torchvision.transforms.ToTensor(), \n",
    "                                     target_transform=None, \n",
    "                                     download=True)\n",
    "mnist_test  = torchvision.datasets.MNIST(\"./\", train=False, \n",
    "                                     transform=torchvision.transforms.ToTensor(), \n",
    "                                     target_transform=None, \n",
    "                                     download=True)\n",
    "\n",
    "# Set dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Network architecture\n",
    "\n",
    "An auto-encoder is network, typically used for *unsupervised* learning, that outputs an image/signal of same dimension of the input one, and is trained to match the input and output.\n",
    "\n",
    "The key elements is that inside the network, it reduces greatly the dimensionallity of the data. As it learns in a given set of images/signals, it tends to remove other type of signals, such as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><figure>\n",
    "    <img src='img/Autoencoder.png' alt='missing' width=\"700\"/>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:**\n",
    "* Train without noise\n",
    "* Use on noisy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(1,32,3,padding=1),   # batch x 32 x 28 x 28\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(32,64,3,padding=1),   # batch x 64 x 14 x 14\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "                        nn.Conv2d(64,128,3,padding=1),   # batch x 128 x 7 x 7\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(128,64,3,2,1,1),\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(64,32,3,2,1,1),\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "                        nn.Conv2d(32,1,3,padding=1), \n",
    "                        nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)      \n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder = nn.Sequential(Encoder(),\n",
    "                            Decoder()\n",
    "                           ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate,weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(autoencoder, device, train_loader, optimizer, epoch):\n",
    "    autoencoder.train()\n",
    "    for batch_idx, (image, target) in enumerate(train_loader):\n",
    "        image,_ = image.to(device), target.to(device)\n",
    "        image_noise = add_noise(image,device)\n",
    "        optimizer.zero_grad()\n",
    "        output = autoencoder(image)\n",
    "        loss = loss_func(output,image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0 and epoch % 5 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(image), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "        if epoch % 10 == 0: # save images\n",
    "            x = image[0:4].cpu().data\n",
    "            x_hat = output[0:4].cpu().data\n",
    "            x_noisy = image_noise[0:4].cpu().data\n",
    "            save_image(x, './x_{}.png'.format(epoch))\n",
    "            save_image(x_hat, './x_hat_{}.png'.format(epoch))\n",
    "            save_image(x_noisy, './x_noisy_{}.png'.format(epoch))\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.114566\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.112328\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.119311\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.118859\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.107526\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.113816\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.112873\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.109902\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.090367\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.103279\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.106274\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 0.089205\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 0.086817\n",
      "Train Epoch: 65 [0/60000 (0%)]\tLoss: 0.079932\n",
      "Train Epoch: 70 [0/60000 (0%)]\tLoss: 0.072040\n",
      "Train Epoch: 75 [0/60000 (0%)]\tLoss: 0.057422\n",
      "Train Epoch: 80 [0/60000 (0%)]\tLoss: 0.046960\n",
      "Train Epoch: 85 [0/60000 (0%)]\tLoss: 0.040940\n",
      "Train Epoch: 90 [0/60000 (0%)]\tLoss: 0.040753\n",
      "Train Epoch: 95 [0/60000 (0%)]\tLoss: 0.034275\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1e10\n",
    "for iepoch in range(epoch):\n",
    "    current_loss = train(autoencoder, device, train_loader, optimizer, iepoch) \n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        # save the best model parameters\n",
    "        torch.save(autoencoder.state_dict(), 'demo_autoencoder.pkl')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def add_noise(inputs):\n",
    "    noise = torch.randn_like(inputs)*0.5\n",
    "    return inputs + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x,_ = mnist_train[20]\n",
    "x_noisy = add_noise(x)[None,...]\n",
    "x_hat = autoencoder(x_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2067032c10>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "noisy_image = x_noisy.detach().cpu().numpy()\n",
    "noisy_image = noisy_image.squeeze()\n",
    "\n",
    "denoised_image = x_hat.detach().cpu().numpy()\n",
    "denoised_image = denoised_image.squeeze()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.imshow(noisy_image, cmap='gray')\n",
    "plt.subplot(212)\n",
    "plt.imshow(denoised_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "**Input**  \n",
    "\n",
    "<figure>\n",
    "    <img src='img/denoise_noisy.png' alt='missing' width=\"400\"/>\n",
    "</figure><br>\n",
    "    \n",
    "**Output**  \n",
    "    \n",
    "<figure>\n",
    "    <img src='img/denoise_hat.png' alt='missing' width=\"400\"/>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loading the model parameters for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Recreate the same model trained\n",
    "model = nn.Sequential(Encoder(),Decoder()).to(device)\n",
    "# load the parameters\n",
    "model.load_state_dict(torch.load('demo_autoencoder.pkl'))\n",
    "# set to eval() for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# use it!\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extending PyTorch\n",
    "\n",
    "More information [here](https://pytorch.org/docs/stable/notes/extending.html) and [here](https://pytorch.org/docs/stable/jit.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we want to define a new cost function to add some penalities, for example to force the output or some parameters to be sparse.\n",
    "When using **only** torch function or operation, it can be done extremely easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this example, we want the weight of the first linear layer to be spares (from [PyTorch forum](https://discuss.pytorch.org/t/l1-regularization-for-a-single-matrix/28088))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 1.0207067728042603, mat0 norm 16.48489761352539\n",
      "Epoch 1, loss 1.0204311609268188, mat0 norm 16.48284912109375\n",
      "Epoch 2, loss 1.020155668258667, mat0 norm 16.480796813964844\n",
      "Epoch 3, loss 1.0198808908462524, mat0 norm 16.478748321533203\n",
      "Epoch 4, loss 1.019606351852417, mat0 norm 16.47669792175293\n",
      "Epoch 5, loss 1.0193331241607666, mat0 norm 16.474639892578125\n",
      "Epoch 6, loss 1.019060492515564, mat0 norm 16.472591400146484\n",
      "Epoch 7, loss 1.0187879800796509, mat0 norm 16.470537185668945\n",
      "Epoch 8, loss 1.0185158252716064, mat0 norm 16.46847152709961\n",
      "Epoch 9, loss 1.0182440280914307, mat0 norm 16.466419219970703\n",
      "Epoch 10, loss 1.0179729461669922, mat0 norm 16.464359283447266\n",
      "Epoch 11, loss 1.0177018642425537, mat0 norm 16.46230697631836\n",
      "Epoch 12, loss 1.0174312591552734, mat0 norm 16.46025276184082\n",
      "Epoch 13, loss 1.017161250114441, mat0 norm 16.45819664001465\n",
      "Epoch 14, loss 1.0168914794921875, mat0 norm 16.456157684326172\n",
      "Epoch 15, loss 1.0166220664978027, mat0 norm 16.454103469848633\n",
      "Epoch 16, loss 1.0163530111312866, mat0 norm 16.452051162719727\n",
      "Epoch 17, loss 1.0160843133926392, mat0 norm 16.450002670288086\n",
      "Epoch 18, loss 1.0158158540725708, mat0 norm 16.447952270507812\n",
      "Epoch 19, loss 1.0155479907989502, mat0 norm 16.445905685424805\n",
      "Epoch 20, loss 1.0152803659439087, mat0 norm 16.443849563598633\n",
      "Epoch 21, loss 1.0150132179260254, mat0 norm 16.441804885864258\n",
      "Epoch 22, loss 1.0147464275360107, mat0 norm 16.439769744873047\n",
      "Epoch 23, loss 1.0144801139831543, mat0 norm 16.437726974487305\n",
      "Epoch 24, loss 1.014214038848877, mat0 norm 16.435678482055664\n",
      "Epoch 25, loss 1.0139483213424683, mat0 norm 16.433635711669922\n",
      "Epoch 26, loss 1.0136830806732178, mat0 norm 16.431589126586914\n",
      "Epoch 27, loss 1.0134178400039673, mat0 norm 16.429548263549805\n",
      "Epoch 28, loss 1.013153314590454, mat0 norm 16.427507400512695\n",
      "Epoch 29, loss 1.0128892660140991, mat0 norm 16.42546272277832\n",
      "Epoch 30, loss 1.0126255750656128, mat0 norm 16.423437118530273\n",
      "Epoch 31, loss 1.0123618841171265, mat0 norm 16.42139434814453\n",
      "Epoch 32, loss 1.0120986700057983, mat0 norm 16.419355392456055\n",
      "Epoch 33, loss 1.0118359327316284, mat0 norm 16.417316436767578\n",
      "Epoch 34, loss 1.0115734338760376, mat0 norm 16.415279388427734\n",
      "Epoch 35, loss 1.0113115310668945, mat0 norm 16.413246154785156\n",
      "Epoch 36, loss 1.0110498666763306, mat0 norm 16.411205291748047\n",
      "Epoch 37, loss 1.0107885599136353, mat0 norm 16.4091854095459\n",
      "Epoch 38, loss 1.0105276107788086, mat0 norm 16.407142639160156\n",
      "Epoch 39, loss 1.0102670192718506, mat0 norm 16.405115127563477\n",
      "Epoch 40, loss 1.0100067853927612, mat0 norm 16.403078079223633\n",
      "Epoch 41, loss 1.009746789932251, mat0 norm 16.401046752929688\n",
      "Epoch 42, loss 1.009487271308899, mat0 norm 16.399015426635742\n",
      "Epoch 43, loss 1.009228229522705, mat0 norm 16.396989822387695\n",
      "Epoch 44, loss 1.0089695453643799, mat0 norm 16.39495849609375\n",
      "Epoch 45, loss 1.0087110996246338, mat0 norm 16.392934799194336\n",
      "Epoch 46, loss 1.0084528923034668, mat0 norm 16.39090919494629\n",
      "Epoch 47, loss 1.008195161819458, mat0 norm 16.38888168334961\n",
      "Epoch 48, loss 1.0079377889633179, mat0 norm 16.38685417175293\n",
      "Epoch 49, loss 1.0076806545257568, mat0 norm 16.384824752807617\n",
      "Epoch 50, loss 1.007423996925354, mat0 norm 16.382802963256836\n",
      "Epoch 51, loss 1.0071676969528198, mat0 norm 16.380775451660156\n",
      "Epoch 52, loss 1.0069118738174438, mat0 norm 16.37876319885254\n",
      "Epoch 53, loss 1.006656289100647, mat0 norm 16.376745223999023\n",
      "Epoch 54, loss 1.0064009428024292, mat0 norm 16.374717712402344\n",
      "Epoch 55, loss 1.00614595413208, mat0 norm 16.37269401550293\n",
      "Epoch 56, loss 1.0058913230895996, mat0 norm 16.37067222595215\n",
      "Epoch 57, loss 1.0056371688842773, mat0 norm 16.368656158447266\n",
      "Epoch 58, loss 1.0053832530975342, mat0 norm 16.366634368896484\n",
      "Epoch 59, loss 1.0051298141479492, mat0 norm 16.364625930786133\n",
      "Epoch 60, loss 1.004876732826233, mat0 norm 16.362611770629883\n",
      "Epoch 61, loss 1.0046238899230957, mat0 norm 16.360595703125\n",
      "Epoch 62, loss 1.0043712854385376, mat0 norm 16.35857582092285\n",
      "Epoch 63, loss 1.0041190385818481, mat0 norm 16.356557846069336\n",
      "Epoch 64, loss 1.003867268562317, mat0 norm 16.35454750061035\n",
      "Epoch 65, loss 1.0036158561706543, mat0 norm 16.35253143310547\n",
      "Epoch 66, loss 1.00336492061615, mat0 norm 16.350528717041016\n",
      "Epoch 67, loss 1.003114104270935, mat0 norm 16.3485164642334\n",
      "Epoch 68, loss 1.0028636455535889, mat0 norm 16.34650230407715\n",
      "Epoch 69, loss 1.0026135444641113, mat0 norm 16.344493865966797\n",
      "Epoch 70, loss 1.002363681793213, mat0 norm 16.342479705810547\n",
      "Epoch 71, loss 1.002114176750183, mat0 norm 16.340471267700195\n",
      "Epoch 72, loss 1.001865267753601, mat0 norm 16.338462829589844\n",
      "Epoch 73, loss 1.0016165971755981, mat0 norm 16.336469650268555\n",
      "Epoch 74, loss 1.0013680458068848, mat0 norm 16.334455490112305\n",
      "Epoch 75, loss 1.0011200904846191, mat0 norm 16.332448959350586\n",
      "Epoch 76, loss 1.0008723735809326, mat0 norm 16.330440521240234\n",
      "Epoch 77, loss 1.0006248950958252, mat0 norm 16.328439712524414\n",
      "Epoch 78, loss 1.000377893447876, mat0 norm 16.326433181762695\n",
      "Epoch 79, loss 1.0001312494277954, mat0 norm 16.324440002441406\n",
      "Epoch 80, loss 0.9998847246170044, mat0 norm 16.322433471679688\n",
      "Epoch 81, loss 0.9996388554573059, mat0 norm 16.320436477661133\n",
      "Epoch 82, loss 0.9993929862976074, mat0 norm 16.318429946899414\n",
      "Epoch 83, loss 0.9991475939750671, mat0 norm 16.316431045532227\n",
      "Epoch 84, loss 0.9989026188850403, mat0 norm 16.314428329467773\n",
      "Epoch 85, loss 0.9986579418182373, mat0 norm 16.31243133544922\n",
      "Epoch 86, loss 0.9984135627746582, mat0 norm 16.310443878173828\n",
      "Epoch 87, loss 0.998169481754303, mat0 norm 16.30844497680664\n",
      "Epoch 88, loss 0.9979256391525269, mat0 norm 16.306446075439453\n",
      "Epoch 89, loss 0.9976822733879089, mat0 norm 16.30445098876953\n",
      "Epoch 90, loss 0.9974391460418701, mat0 norm 16.302453994750977\n",
      "Epoch 91, loss 0.9971963763237, mat0 norm 16.300458908081055\n",
      "Epoch 92, loss 0.9969541430473328, mat0 norm 16.298473358154297\n",
      "Epoch 93, loss 0.9967120289802551, mat0 norm 16.29648208618164\n",
      "Epoch 94, loss 0.9964701533317566, mat0 norm 16.294485092163086\n",
      "Epoch 95, loss 0.9962285757064819, mat0 norm 16.292497634887695\n",
      "Epoch 96, loss 0.9959874153137207, mat0 norm 16.290504455566406\n",
      "Epoch 97, loss 0.9957466721534729, mat0 norm 16.288511276245117\n",
      "Epoch 98, loss 0.9955061674118042, mat0 norm 16.286518096923828\n",
      "Epoch 99, loss 0.9952661395072937, mat0 norm 16.284543991088867\n",
      "Epoch 100, loss 0.9950263500213623, mat0 norm 16.28255271911621\n",
      "Epoch 101, loss 0.9947867393493652, mat0 norm 16.28056526184082\n",
      "Epoch 102, loss 0.9945474863052368, mat0 norm 16.278573989868164\n",
      "Epoch 103, loss 0.9943086504936218, mat0 norm 16.276586532592773\n",
      "Epoch 104, loss 0.9940700531005859, mat0 norm 16.27460479736328\n",
      "Epoch 105, loss 0.9938319325447083, mat0 norm 16.272632598876953\n",
      "Epoch 106, loss 0.9935940504074097, mat0 norm 16.27064323425293\n",
      "Epoch 107, loss 0.9933563470840454, mat0 norm 16.268661499023438\n",
      "Epoch 108, loss 0.9931188821792603, mat0 norm 16.26667594909668\n",
      "Epoch 109, loss 0.9928819537162781, mat0 norm 16.264694213867188\n",
      "Epoch 110, loss 0.9926453232765198, mat0 norm 16.262710571289062\n",
      "Epoch 111, loss 0.992408812046051, mat0 norm 16.260738372802734\n",
      "Epoch 112, loss 0.9921545386314392, mat0 norm 16.25868034362793\n",
      "Epoch 113, loss 0.9919005036354065, mat0 norm 16.256624221801758\n",
      "Epoch 114, loss 0.9916467070579529, mat0 norm 16.25456428527832\n",
      "Epoch 115, loss 0.9913935661315918, mat0 norm 16.25250816345215\n",
      "Epoch 116, loss 0.991140604019165, mat0 norm 16.250455856323242\n",
      "Epoch 117, loss 0.9908881783485413, mat0 norm 16.24840545654297\n",
      "Epoch 118, loss 0.9906359910964966, mat0 norm 16.246349334716797\n",
      "Epoch 119, loss 0.9903841614723206, mat0 norm 16.24430274963379\n",
      "Epoch 120, loss 0.990132749080658, mat0 norm 16.242246627807617\n",
      "Epoch 121, loss 0.9898816347122192, mat0 norm 16.240196228027344\n",
      "Epoch 122, loss 0.9896308183670044, mat0 norm 16.238143920898438\n",
      "Epoch 123, loss 0.9893806576728821, mat0 norm 16.236108779907227\n",
      "Epoch 124, loss 0.9891306161880493, mat0 norm 16.23405647277832\n",
      "Epoch 125, loss 0.9888808727264404, mat0 norm 16.232004165649414\n",
      "Epoch 126, loss 0.9886316061019897, mat0 norm 16.22995376586914\n",
      "Epoch 127, loss 0.9883826375007629, mat0 norm 16.227909088134766\n",
      "Epoch 128, loss 0.9881340861320496, mat0 norm 16.225862503051758\n",
      "Epoch 129, loss 0.9878859519958496, mat0 norm 16.223827362060547\n",
      "Epoch 130, loss 0.987637996673584, mat0 norm 16.221782684326172\n",
      "Epoch 131, loss 0.9873905181884766, mat0 norm 16.21973991394043\n",
      "Epoch 132, loss 0.9871432185173035, mat0 norm 16.21769142150879\n",
      "Epoch 133, loss 0.9868963360786438, mat0 norm 16.21565055847168\n",
      "Epoch 134, loss 0.9866499900817871, mat0 norm 16.213607788085938\n",
      "Epoch 135, loss 0.9864039421081543, mat0 norm 16.211576461791992\n",
      "Epoch 136, loss 0.9861581325531006, mat0 norm 16.20953941345215\n",
      "Epoch 137, loss 0.9859126210212708, mat0 norm 16.207496643066406\n",
      "Epoch 138, loss 0.9856675863265991, mat0 norm 16.205459594726562\n",
      "Epoch 139, loss 0.9854227304458618, mat0 norm 16.203418731689453\n",
      "Epoch 140, loss 0.9851785898208618, mat0 norm 16.201383590698242\n",
      "Epoch 141, loss 0.9849345088005066, mat0 norm 16.199356079101562\n",
      "Epoch 142, loss 0.98469078540802, mat0 norm 16.19732093811035\n",
      "Epoch 143, loss 0.9844474792480469, mat0 norm 16.195283889770508\n",
      "Epoch 144, loss 0.9842042922973633, mat0 norm 16.19325065612793\n",
      "Epoch 145, loss 0.9839615821838379, mat0 norm 16.191213607788086\n",
      "Epoch 146, loss 0.983719527721405, mat0 norm 16.189193725585938\n",
      "Epoch 147, loss 0.9834773540496826, mat0 norm 16.187158584594727\n",
      "Epoch 148, loss 0.9832358360290527, mat0 norm 16.18512725830078\n",
      "Epoch 149, loss 0.9829944372177124, mat0 norm 16.18309783935547\n",
      "Epoch 150, loss 0.9827534556388855, mat0 norm 16.181068420410156\n",
      "Epoch 151, loss 0.9825130105018616, mat0 norm 16.17904281616211\n",
      "Epoch 152, loss 0.982272744178772, mat0 norm 16.177019119262695\n",
      "Epoch 153, loss 0.9820327162742615, mat0 norm 16.174991607666016\n",
      "Epoch 154, loss 0.9817930459976196, mat0 norm 16.172962188720703\n",
      "Epoch 155, loss 0.9815537333488464, mat0 norm 16.17093849182129\n",
      "Epoch 156, loss 0.9813147783279419, mat0 norm 16.168912887573242\n",
      "Epoch 157, loss 0.9810764193534851, mat0 norm 16.16688346862793\n",
      "Epoch 158, loss 0.9808380603790283, mat0 norm 16.164873123168945\n",
      "Epoch 159, loss 0.980600118637085, mat0 norm 16.1628475189209\n",
      "Epoch 160, loss 0.9803622961044312, mat0 norm 16.160825729370117\n",
      "Epoch 161, loss 0.9801251292228699, mat0 norm 16.158802032470703\n",
      "Epoch 162, loss 0.9798882007598877, mat0 norm 16.156784057617188\n",
      "Epoch 163, loss 0.979651689529419, mat0 norm 16.15477180480957\n",
      "Epoch 164, loss 0.9794154167175293, mat0 norm 16.152753829956055\n",
      "Epoch 165, loss 0.9791793823242188, mat0 norm 16.150733947753906\n",
      "Epoch 166, loss 0.9789437055587769, mat0 norm 16.148714065551758\n",
      "Epoch 167, loss 0.9787084460258484, mat0 norm 16.146696090698242\n",
      "Epoch 168, loss 0.9784736037254333, mat0 norm 16.14468002319336\n",
      "Epoch 169, loss 0.9782389402389526, mat0 norm 16.142675399780273\n",
      "Epoch 170, loss 0.978004515171051, mat0 norm 16.14065933227539\n",
      "Epoch 171, loss 0.9777704477310181, mat0 norm 16.138641357421875\n",
      "Epoch 172, loss 0.9775367975234985, mat0 norm 16.136625289916992\n",
      "Epoch 173, loss 0.9773035049438477, mat0 norm 16.134614944458008\n",
      "Epoch 174, loss 0.977070689201355, mat0 norm 16.132614135742188\n",
      "Epoch 175, loss 0.9768379330635071, mat0 norm 16.130603790283203\n",
      "Epoch 176, loss 0.9766054153442383, mat0 norm 16.128591537475586\n",
      "Epoch 177, loss 0.9763733148574829, mat0 norm 16.126577377319336\n",
      "Epoch 178, loss 0.9761416912078857, mat0 norm 16.124568939208984\n",
      "Epoch 179, loss 0.9759104251861572, mat0 norm 16.12257194519043\n",
      "Epoch 180, loss 0.9756791591644287, mat0 norm 16.120561599731445\n",
      "Epoch 181, loss 0.9754483699798584, mat0 norm 16.118558883666992\n",
      "Epoch 182, loss 0.975217878818512, mat0 norm 16.116546630859375\n",
      "Epoch 183, loss 0.9749876856803894, mat0 norm 16.114538192749023\n",
      "Epoch 184, loss 0.974757969379425, mat0 norm 16.112546920776367\n",
      "Epoch 185, loss 0.974528431892395, mat0 norm 16.110538482666016\n",
      "Epoch 186, loss 0.9742991924285889, mat0 norm 16.108535766601562\n",
      "Epoch 187, loss 0.9740703701972961, mat0 norm 16.106529235839844\n",
      "Epoch 188, loss 0.9738416075706482, mat0 norm 16.104530334472656\n",
      "Epoch 189, loss 0.9736136198043823, mat0 norm 16.10252571105957\n",
      "Epoch 190, loss 0.9733855724334717, mat0 norm 16.100536346435547\n",
      "Epoch 191, loss 0.9731579422950745, mat0 norm 16.09853744506836\n",
      "Epoch 192, loss 0.9729305505752563, mat0 norm 16.09653663635254\n",
      "Epoch 193, loss 0.9727035760879517, mat0 norm 16.094539642333984\n",
      "Epoch 194, loss 0.9724770784378052, mat0 norm 16.092538833618164\n",
      "Epoch 195, loss 0.9722505211830139, mat0 norm 16.090553283691406\n",
      "Epoch 196, loss 0.9720244407653809, mat0 norm 16.088552474975586\n",
      "Epoch 197, loss 0.9717985987663269, mat0 norm 16.08656120300293\n",
      "Epoch 198, loss 0.9715731143951416, mat0 norm 16.08456039428711\n",
      "Epoch 199, loss 0.9713480472564697, mat0 norm 16.082569122314453\n",
      "Epoch 200, loss 0.971123218536377, mat0 norm 16.080581665039062\n",
      "Epoch 201, loss 0.9708987474441528, mat0 norm 16.07859230041504\n",
      "Epoch 202, loss 0.9706743955612183, mat0 norm 16.076597213745117\n",
      "Epoch 203, loss 0.9704504609107971, mat0 norm 16.07460594177246\n",
      "Epoch 204, loss 0.9702268838882446, mat0 norm 16.07261085510254\n",
      "Epoch 205, loss 0.970003604888916, mat0 norm 16.070632934570312\n",
      "Epoch 206, loss 0.9697805047035217, mat0 norm 16.068639755249023\n",
      "Epoch 207, loss 0.9695577025413513, mat0 norm 16.066652297973633\n",
      "Epoch 208, loss 0.9693352580070496, mat0 norm 16.06466293334961\n",
      "Epoch 209, loss 0.9691133499145508, mat0 norm 16.062673568725586\n",
      "Epoch 210, loss 0.9688916206359863, mat0 norm 16.060701370239258\n",
      "Epoch 211, loss 0.9686698913574219, mat0 norm 16.058712005615234\n",
      "Epoch 212, loss 0.9684486389160156, mat0 norm 16.056718826293945\n",
      "Epoch 213, loss 0.9682276248931885, mat0 norm 16.05474090576172\n",
      "Epoch 214, loss 0.9680070877075195, mat0 norm 16.052753448486328\n",
      "Epoch 215, loss 0.9677867889404297, mat0 norm 16.050783157348633\n",
      "Epoch 216, loss 0.9675666689872742, mat0 norm 16.048795700073242\n",
      "Epoch 217, loss 0.9673468470573425, mat0 norm 16.046810150146484\n",
      "Epoch 218, loss 0.9671275019645691, mat0 norm 16.044836044311523\n",
      "Epoch 219, loss 0.9669085144996643, mat0 norm 16.042848587036133\n",
      "Epoch 220, loss 0.9666895866394043, mat0 norm 16.04088020324707\n",
      "Epoch 221, loss 0.9664710760116577, mat0 norm 16.03890037536621\n",
      "Epoch 222, loss 0.9662526249885559, mat0 norm 16.03692054748535\n",
      "Epoch 223, loss 0.9660346508026123, mat0 norm 16.034944534301758\n",
      "Epoch 224, loss 0.9658172726631165, mat0 norm 16.0329647064209\n",
      "Epoch 225, loss 0.9655998945236206, mat0 norm 16.0310001373291\n",
      "Epoch 226, loss 0.9653826355934143, mat0 norm 16.02901840209961\n",
      "Epoch 227, loss 0.9651656746864319, mat0 norm 16.02704429626465\n",
      "Epoch 228, loss 0.9649491310119629, mat0 norm 16.025070190429688\n",
      "Epoch 229, loss 0.9647330641746521, mat0 norm 16.023107528686523\n",
      "Epoch 230, loss 0.9645172357559204, mat0 norm 16.021135330200195\n",
      "Epoch 231, loss 0.9643014669418335, mat0 norm 16.01915740966797\n",
      "Epoch 232, loss 0.9640858769416809, mat0 norm 16.017179489135742\n",
      "Epoch 233, loss 0.9638710021972656, mat0 norm 16.01521110534668\n",
      "Epoch 234, loss 0.9636561870574951, mat0 norm 16.013254165649414\n",
      "Epoch 235, loss 0.9634416103363037, mat0 norm 16.011281967163086\n",
      "Epoch 236, loss 0.9632274508476257, mat0 norm 16.00931167602539\n",
      "Epoch 237, loss 0.9630132913589478, mat0 norm 16.007341384887695\n",
      "Epoch 238, loss 0.9627998471260071, mat0 norm 16.005373001098633\n",
      "Epoch 239, loss 0.962586522102356, mat0 norm 16.003416061401367\n",
      "Epoch 240, loss 0.9623732566833496, mat0 norm 16.001447677612305\n",
      "Epoch 241, loss 0.9621605277061462, mat0 norm 15.999485969543457\n",
      "Epoch 242, loss 0.9619478583335876, mat0 norm 15.997515678405762\n",
      "Epoch 243, loss 0.9617358446121216, mat0 norm 15.995552062988281\n",
      "Epoch 244, loss 0.9615238308906555, mat0 norm 15.993597030639648\n",
      "Epoch 245, loss 0.9613120555877686, mat0 norm 15.991630554199219\n",
      "Epoch 246, loss 0.9611005783081055, mat0 norm 15.989670753479004\n",
      "Epoch 247, loss 0.9608895778656006, mat0 norm 15.98770809173584\n",
      "Epoch 248, loss 0.96067875623703, mat0 norm 15.985756874084473\n",
      "Epoch 249, loss 0.9604681730270386, mat0 norm 15.983795166015625\n",
      "Epoch 250, loss 0.960257887840271, mat0 norm 15.981833457946777\n",
      "Epoch 251, loss 0.9600476026535034, mat0 norm 15.979870796203613\n",
      "Epoch 252, loss 0.9598380327224731, mat0 norm 15.977909088134766\n",
      "Epoch 253, loss 0.9596285820007324, mat0 norm 15.97596549987793\n",
      "Epoch 254, loss 0.9594194293022156, mat0 norm 15.974010467529297\n",
      "Epoch 255, loss 0.9592102766036987, mat0 norm 15.9720458984375\n",
      "Epoch 256, loss 0.9590016603469849, mat0 norm 15.970090866088867\n",
      "Epoch 257, loss 0.9587935209274292, mat0 norm 15.968148231506348\n",
      "Epoch 258, loss 0.9585853219032288, mat0 norm 15.966195106506348\n",
      "Epoch 259, loss 0.9583774209022522, mat0 norm 15.964239120483398\n",
      "Epoch 260, loss 0.95816969871521, mat0 norm 15.962282180786133\n",
      "Epoch 261, loss 0.9579626321792603, mat0 norm 15.96033000946045\n",
      "Epoch 262, loss 0.9577556252479553, mat0 norm 15.95838737487793\n",
      "Epoch 263, loss 0.9575487971305847, mat0 norm 15.956432342529297\n",
      "Epoch 264, loss 0.957342267036438, mat0 norm 15.954480171203613\n",
      "Epoch 265, loss 0.9571360349655151, mat0 norm 15.952530860900879\n",
      "Epoch 266, loss 0.9569301009178162, mat0 norm 15.950592041015625\n",
      "Epoch 267, loss 0.9567244648933411, mat0 norm 15.948640823364258\n",
      "Epoch 268, loss 0.9565190672874451, mat0 norm 15.946693420410156\n",
      "Epoch 269, loss 0.9563137888908386, mat0 norm 15.94473934173584\n",
      "Epoch 270, loss 0.9561090469360352, mat0 norm 15.942797660827637\n",
      "Epoch 271, loss 0.955904483795166, mat0 norm 15.94085693359375\n",
      "Epoch 272, loss 0.9557000994682312, mat0 norm 15.938913345336914\n",
      "Epoch 273, loss 0.9554958939552307, mat0 norm 15.936965942382812\n",
      "Epoch 274, loss 0.9552920460700989, mat0 norm 15.93502140045166\n",
      "Epoch 275, loss 0.9550886154174805, mat0 norm 15.933087348937988\n",
      "Epoch 276, loss 0.9548853039741516, mat0 norm 15.931143760681152\n",
      "Epoch 277, loss 0.9546822309494019, mat0 norm 15.929198265075684\n",
      "Epoch 278, loss 0.9544793367385864, mat0 norm 15.927255630493164\n",
      "Epoch 279, loss 0.9542769193649292, mat0 norm 15.925310134887695\n",
      "Epoch 280, loss 0.9540747404098511, mat0 norm 15.923386573791504\n",
      "Epoch 281, loss 0.9538725018501282, mat0 norm 15.921440124511719\n",
      "Epoch 282, loss 0.953670859336853, mat0 norm 15.919500350952148\n",
      "Epoch 283, loss 0.9534693956375122, mat0 norm 15.917562484741211\n",
      "Epoch 284, loss 0.9532684087753296, mat0 norm 15.91563606262207\n",
      "Epoch 285, loss 0.9530671834945679, mat0 norm 15.9136962890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286, loss 0.9528664946556091, mat0 norm 15.91175651550293\n",
      "Epoch 287, loss 0.9526659250259399, mat0 norm 15.90981674194336\n",
      "Epoch 288, loss 0.9524660706520081, mat0 norm 15.907882690429688\n",
      "Epoch 289, loss 0.9522659778594971, mat0 norm 15.905956268310547\n",
      "Epoch 290, loss 0.9520662426948547, mat0 norm 15.90402603149414\n",
      "Epoch 291, loss 0.9518667459487915, mat0 norm 15.902087211608887\n",
      "Epoch 292, loss 0.9516676664352417, mat0 norm 15.900148391723633\n",
      "Epoch 293, loss 0.951468825340271, mat0 norm 15.898232460021973\n",
      "Epoch 294, loss 0.9512700438499451, mat0 norm 15.896302223205566\n",
      "Epoch 295, loss 0.9510716199874878, mat0 norm 15.894362449645996\n",
      "Epoch 296, loss 0.9508734345436096, mat0 norm 15.892433166503906\n",
      "Epoch 297, loss 0.9506756663322449, mat0 norm 15.89051342010498\n",
      "Epoch 298, loss 0.9504778981208801, mat0 norm 15.888583183288574\n",
      "Epoch 299, loss 0.9502804279327393, mat0 norm 15.886650085449219\n",
      "Epoch 300, loss 0.9500832557678223, mat0 norm 15.884720802307129\n",
      "Epoch 301, loss 0.9498865604400635, mat0 norm 15.882806777954102\n",
      "Epoch 302, loss 0.9496897459030151, mat0 norm 15.880878448486328\n",
      "Epoch 303, loss 0.9494932889938354, mat0 norm 15.878949165344238\n",
      "Epoch 304, loss 0.9492969512939453, mat0 norm 15.877022743225098\n",
      "Epoch 305, loss 0.9491012692451477, mat0 norm 15.875097274780273\n",
      "Epoch 306, loss 0.9489056468009949, mat0 norm 15.873186111450195\n",
      "Epoch 307, loss 0.9487101435661316, mat0 norm 15.871261596679688\n",
      "Epoch 308, loss 0.9485148787498474, mat0 norm 15.869336128234863\n",
      "Epoch 309, loss 0.9483201503753662, mat0 norm 15.867411613464355\n",
      "Epoch 310, loss 0.9481253623962402, mat0 norm 15.865497589111328\n",
      "Epoch 311, loss 0.9479309320449829, mat0 norm 15.863576889038086\n",
      "Epoch 312, loss 0.9477365612983704, mat0 norm 15.861648559570312\n",
      "Epoch 313, loss 0.947542667388916, mat0 norm 15.85972785949707\n",
      "Epoch 314, loss 0.9473490715026855, mat0 norm 15.85782241821289\n",
      "Epoch 315, loss 0.9471555352210999, mat0 norm 15.855903625488281\n",
      "Epoch 316, loss 0.946962296962738, mat0 norm 15.853983879089355\n",
      "Epoch 317, loss 0.9467692375183105, mat0 norm 15.85206127166748\n",
      "Epoch 318, loss 0.9465765953063965, mat0 norm 15.850156784057617\n",
      "Epoch 319, loss 0.946384072303772, mat0 norm 15.848238945007324\n",
      "Epoch 320, loss 0.946191668510437, mat0 norm 15.846319198608398\n",
      "Epoch 321, loss 0.9459996223449707, mat0 norm 15.844405174255371\n",
      "Epoch 322, loss 0.9458080530166626, mat0 norm 15.842499732971191\n",
      "Epoch 323, loss 0.9456164836883545, mat0 norm 15.84058952331543\n",
      "Epoch 324, loss 0.9454249739646912, mat0 norm 15.838665962219238\n",
      "Epoch 325, loss 0.9452338218688965, mat0 norm 15.836753845214844\n",
      "Epoch 326, loss 0.9450433254241943, mat0 norm 15.834855079650879\n",
      "Epoch 327, loss 0.9448525905609131, mat0 norm 15.832937240600586\n",
      "Epoch 328, loss 0.9446620345115662, mat0 norm 15.831025123596191\n",
      "Epoch 329, loss 0.9444719552993774, mat0 norm 15.829113960266113\n",
      "Epoch 330, loss 0.9442822337150574, mat0 norm 15.82719898223877\n",
      "Epoch 331, loss 0.9440925121307373, mat0 norm 15.82530403137207\n",
      "Epoch 332, loss 0.9439030885696411, mat0 norm 15.823391914367676\n",
      "Epoch 333, loss 0.9437136650085449, mat0 norm 15.821483612060547\n",
      "Epoch 334, loss 0.943524956703186, mat0 norm 15.819575309753418\n",
      "Epoch 335, loss 0.9433363080024719, mat0 norm 15.817684173583984\n",
      "Epoch 336, loss 0.9431476593017578, mat0 norm 15.81576919555664\n",
      "Epoch 337, loss 0.9429593086242676, mat0 norm 15.813863754272461\n",
      "Epoch 338, loss 0.9427714347839355, mat0 norm 15.8119535446167\n",
      "Epoch 339, loss 0.9425836205482483, mat0 norm 15.810060501098633\n",
      "Epoch 340, loss 0.9423959255218506, mat0 norm 15.808155059814453\n",
      "Epoch 341, loss 0.9422085285186768, mat0 norm 15.806248664855957\n",
      "Epoch 342, loss 0.9420214891433716, mat0 norm 15.804342269897461\n",
      "Epoch 343, loss 0.9418346881866455, mat0 norm 15.802454948425293\n",
      "Epoch 344, loss 0.9416481256484985, mat0 norm 15.800565719604492\n",
      "Epoch 345, loss 0.9414634108543396, mat0 norm 15.798751831054688\n",
      "Epoch 346, loss 0.9412792921066284, mat0 norm 15.796942710876465\n",
      "Epoch 347, loss 0.9410951137542725, mat0 norm 15.795145988464355\n",
      "Epoch 348, loss 0.940911054611206, mat0 norm 15.793335914611816\n",
      "Epoch 349, loss 0.9407272934913635, mat0 norm 15.791529655456543\n",
      "Epoch 350, loss 0.9405440092086792, mat0 norm 15.789719581604004\n",
      "Epoch 351, loss 0.9403607845306396, mat0 norm 15.787923812866211\n",
      "Epoch 352, loss 0.9401776194572449, mat0 norm 15.786115646362305\n",
      "Epoch 353, loss 0.9399948120117188, mat0 norm 15.784309387207031\n",
      "Epoch 354, loss 0.9398126602172852, mat0 norm 15.782522201538086\n",
      "Epoch 355, loss 0.9396311044692993, mat0 norm 15.780777931213379\n",
      "Epoch 356, loss 0.9394499063491821, mat0 norm 15.779021263122559\n",
      "Epoch 357, loss 0.9392688870429993, mat0 norm 15.777266502380371\n",
      "Epoch 358, loss 0.9390883445739746, mat0 norm 15.77550983428955\n",
      "Epoch 359, loss 0.9389079213142395, mat0 norm 15.773767471313477\n",
      "Epoch 360, loss 0.9387274980545044, mat0 norm 15.772015571594238\n",
      "Epoch 361, loss 0.9385473728179932, mat0 norm 15.770261764526367\n",
      "Epoch 362, loss 0.9383677244186401, mat0 norm 15.76852035522461\n",
      "Epoch 363, loss 0.9381879568099976, mat0 norm 15.766762733459473\n",
      "Epoch 364, loss 0.9380085468292236, mat0 norm 15.7650146484375\n",
      "Epoch 365, loss 0.9378292560577393, mat0 norm 15.763261795043945\n",
      "Epoch 366, loss 0.9376504421234131, mat0 norm 15.761524200439453\n",
      "Epoch 367, loss 0.9374715685844421, mat0 norm 15.759774208068848\n",
      "Epoch 368, loss 0.9372928738594055, mat0 norm 15.758021354675293\n",
      "Epoch 369, loss 0.9371145367622375, mat0 norm 15.756267547607422\n",
      "Epoch 370, loss 0.936936616897583, mat0 norm 15.754531860351562\n",
      "Epoch 371, loss 0.9367586970329285, mat0 norm 15.752788543701172\n",
      "Epoch 372, loss 0.9365808963775635, mat0 norm 15.75103759765625\n",
      "Epoch 373, loss 0.9364033937454224, mat0 norm 15.749288558959961\n",
      "Epoch 374, loss 0.9362263679504395, mat0 norm 15.747556686401367\n",
      "Epoch 375, loss 0.9360491037368774, mat0 norm 15.745808601379395\n",
      "Epoch 376, loss 0.9358721375465393, mat0 norm 15.744058609008789\n",
      "Epoch 377, loss 0.9356956481933594, mat0 norm 15.74231243133545\n",
      "Epoch 378, loss 0.9355192184448242, mat0 norm 15.740580558776855\n",
      "Epoch 379, loss 0.9353427886962891, mat0 norm 15.738832473754883\n",
      "Epoch 380, loss 0.9351668357849121, mat0 norm 15.737091064453125\n",
      "Epoch 381, loss 0.934991180896759, mat0 norm 15.735345840454102\n",
      "Epoch 382, loss 0.934815526008606, mat0 norm 15.73361587524414\n",
      "Epoch 383, loss 0.934640109539032, mat0 norm 15.731876373291016\n",
      "Epoch 384, loss 0.9344648718833923, mat0 norm 15.73012924194336\n",
      "Epoch 385, loss 0.9342900514602661, mat0 norm 15.728385925292969\n",
      "Epoch 386, loss 0.9341152906417847, mat0 norm 15.726659774780273\n",
      "Epoch 387, loss 0.933940589427948, mat0 norm 15.724913597106934\n",
      "Epoch 388, loss 0.9337661266326904, mat0 norm 15.723175048828125\n",
      "Epoch 389, loss 0.9335921406745911, mat0 norm 15.72144603729248\n",
      "Epoch 390, loss 0.9334181547164917, mat0 norm 15.719708442687988\n",
      "Epoch 391, loss 0.9332442283630371, mat0 norm 15.717968940734863\n",
      "Epoch 392, loss 0.9330707788467407, mat0 norm 15.716228485107422\n",
      "Epoch 393, loss 0.932897686958313, mat0 norm 15.714503288269043\n",
      "Epoch 394, loss 0.9327244758605957, mat0 norm 15.712766647338867\n",
      "Epoch 395, loss 0.9325513243675232, mat0 norm 15.711027145385742\n",
      "Epoch 396, loss 0.9323787689208984, mat0 norm 15.709293365478516\n",
      "Epoch 397, loss 0.9322062730789185, mat0 norm 15.70756721496582\n",
      "Epoch 398, loss 0.932033896446228, mat0 norm 15.705831527709961\n",
      "Epoch 399, loss 0.9318616390228271, mat0 norm 15.704093933105469\n",
      "Epoch 400, loss 0.9316899180412292, mat0 norm 15.70235538482666\n",
      "Epoch 401, loss 0.9315181374549866, mat0 norm 15.700638771057129\n",
      "Epoch 402, loss 0.9313465356826782, mat0 norm 15.69890308380127\n",
      "Epoch 403, loss 0.931175172328949, mat0 norm 15.697164535522461\n",
      "Epoch 404, loss 0.9310042262077332, mat0 norm 15.69544506072998\n",
      "Epoch 405, loss 0.9308332800865173, mat0 norm 15.693711280822754\n",
      "Epoch 406, loss 0.9306623935699463, mat0 norm 15.691980361938477\n",
      "Epoch 407, loss 0.9304919838905334, mat0 norm 15.6902494430542\n",
      "Epoch 408, loss 0.9303216934204102, mat0 norm 15.688529968261719\n",
      "Epoch 409, loss 0.9301515817642212, mat0 norm 15.686799049377441\n",
      "Epoch 410, loss 0.9299814701080322, mat0 norm 15.685068130493164\n",
      "Epoch 411, loss 0.929811954498291, mat0 norm 15.683338165283203\n",
      "Epoch 412, loss 0.9296424388885498, mat0 norm 15.68161678314209\n",
      "Epoch 413, loss 0.929472804069519, mat0 norm 15.679887771606445\n",
      "Epoch 414, loss 0.9293037056922913, mat0 norm 15.678159713745117\n",
      "Epoch 415, loss 0.9291350245475769, mat0 norm 15.6764497756958\n",
      "Epoch 416, loss 0.928966224193573, mat0 norm 15.67471694946289\n",
      "Epoch 417, loss 0.9287974238395691, mat0 norm 15.672987937927246\n",
      "Epoch 418, loss 0.9286291003227234, mat0 norm 15.671258926391602\n",
      "Epoch 419, loss 0.9284610748291016, mat0 norm 15.669548034667969\n",
      "Epoch 420, loss 0.928292989730835, mat0 norm 15.667826652526855\n",
      "Epoch 421, loss 0.9281250238418579, mat0 norm 15.666094779968262\n",
      "Epoch 422, loss 0.9279577136039734, mat0 norm 15.66436767578125\n",
      "Epoch 423, loss 0.9277902245521545, mat0 norm 15.662656784057617\n",
      "Epoch 424, loss 0.92762291431427, mat0 norm 15.660930633544922\n",
      "Epoch 425, loss 0.9274557828903198, mat0 norm 15.659208297729492\n",
      "Epoch 426, loss 0.9272891283035278, mat0 norm 15.65749740600586\n",
      "Epoch 427, loss 0.9271224737167358, mat0 norm 15.655776023864746\n",
      "Epoch 428, loss 0.9269558787345886, mat0 norm 15.65404987335205\n",
      "Epoch 429, loss 0.9267897605895996, mat0 norm 15.652329444885254\n",
      "Epoch 430, loss 0.9266237020492554, mat0 norm 15.650622367858887\n",
      "Epoch 431, loss 0.9264577627182007, mat0 norm 15.64889907836914\n",
      "Epoch 432, loss 0.9262919425964355, mat0 norm 15.647177696228027\n",
      "Epoch 433, loss 0.9261267185211182, mat0 norm 15.645467758178711\n",
      "Epoch 434, loss 0.9259612560272217, mat0 norm 15.64374828338623\n",
      "Epoch 435, loss 0.9257959723472595, mat0 norm 15.642027854919434\n",
      "Epoch 436, loss 0.9256311655044556, mat0 norm 15.640308380126953\n",
      "Epoch 437, loss 0.9254664182662964, mat0 norm 15.638604164123535\n",
      "Epoch 438, loss 0.925301730632782, mat0 norm 15.636886596679688\n",
      "Epoch 439, loss 0.9251371622085571, mat0 norm 15.635164260864258\n",
      "Epoch 440, loss 0.9249732494354248, mat0 norm 15.633453369140625\n",
      "Epoch 441, loss 0.9248090982437134, mat0 norm 15.631743431091309\n",
      "Epoch 442, loss 0.924645185470581, mat0 norm 15.630029678344727\n",
      "Epoch 443, loss 0.9244815111160278, mat0 norm 15.628307342529297\n",
      "Epoch 444, loss 0.9243183135986328, mat0 norm 15.626612663269043\n",
      "Epoch 445, loss 0.9241547584533691, mat0 norm 15.624895095825195\n",
      "Epoch 446, loss 0.9239916205406189, mat0 norm 15.623181343078613\n",
      "Epoch 447, loss 0.9238289594650269, mat0 norm 15.621468544006348\n",
      "Epoch 448, loss 0.9236661195755005, mat0 norm 15.61976432800293\n",
      "Epoch 449, loss 0.9235033988952637, mat0 norm 15.618046760559082\n",
      "Epoch 450, loss 0.9233410954475403, mat0 norm 15.616337776184082\n",
      "Epoch 451, loss 0.923179030418396, mat0 norm 15.614638328552246\n",
      "Epoch 452, loss 0.9230169057846069, mat0 norm 15.612919807434082\n",
      "Epoch 453, loss 0.9228549599647522, mat0 norm 15.611212730407715\n",
      "Epoch 454, loss 0.9226934909820557, mat0 norm 15.6094970703125\n",
      "Epoch 455, loss 0.9225319623947144, mat0 norm 15.607803344726562\n",
      "Epoch 456, loss 0.9223705530166626, mat0 norm 15.606094360351562\n",
      "Epoch 457, loss 0.9222094416618347, mat0 norm 15.604378700256348\n",
      "Epoch 458, loss 0.9220486879348755, mat0 norm 15.60268497467041\n",
      "Epoch 459, loss 0.9218878746032715, mat0 norm 15.60097885131836\n",
      "Epoch 460, loss 0.9217270612716675, mat0 norm 15.599265098571777\n",
      "Epoch 461, loss 0.921566903591156, mat0 norm 15.59755802154541\n",
      "Epoch 462, loss 0.921406626701355, mat0 norm 15.595863342285156\n",
      "Epoch 463, loss 0.9212465286254883, mat0 norm 15.594160079956055\n",
      "Epoch 464, loss 0.9210866689682007, mat0 norm 15.592448234558105\n",
      "Epoch 465, loss 0.9209271669387817, mat0 norm 15.590757369995117\n",
      "Epoch 466, loss 0.9207674264907837, mat0 norm 15.589048385620117\n",
      "Epoch 467, loss 0.9206079244613647, mat0 norm 15.58734130859375\n",
      "Epoch 468, loss 0.9204491376876831, mat0 norm 15.585636138916016\n",
      "Epoch 469, loss 0.9202898144721985, mat0 norm 15.583940505981445\n",
      "Epoch 470, loss 0.9201309680938721, mat0 norm 15.582242012023926\n",
      "Epoch 471, loss 0.9199724197387695, mat0 norm 15.580534934997559\n",
      "Epoch 472, loss 0.9198141098022461, mat0 norm 15.578845977783203\n",
      "Epoch 473, loss 0.9196555614471436, mat0 norm 15.577139854431152\n",
      "Epoch 474, loss 0.9194973707199097, mat0 norm 15.57543659210205\n",
      "Epoch 475, loss 0.9193397164344788, mat0 norm 15.573750495910645\n",
      "Epoch 476, loss 0.9191818237304688, mat0 norm 15.572050094604492\n",
      "Epoch 477, loss 0.9190240502357483, mat0 norm 15.570344924926758\n",
      "Epoch 478, loss 0.918866753578186, mat0 norm 15.568641662597656\n",
      "Epoch 479, loss 0.9187095165252686, mat0 norm 15.566957473754883\n",
      "Epoch 480, loss 0.9185522198677063, mat0 norm 15.565252304077148\n",
      "Epoch 481, loss 0.9183953404426575, mat0 norm 15.563554763793945\n",
      "Epoch 482, loss 0.9182387590408325, mat0 norm 15.561869621276855\n",
      "Epoch 483, loss 0.918082058429718, mat0 norm 15.560166358947754\n",
      "Epoch 484, loss 0.9179255366325378, mat0 norm 15.558467864990234\n",
      "Epoch 485, loss 0.9177694916725159, mat0 norm 15.556770324707031\n",
      "Epoch 486, loss 0.9176133275032043, mat0 norm 15.555084228515625\n",
      "Epoch 487, loss 0.9174573421478271, mat0 norm 15.553391456604004\n",
      "Epoch 488, loss 0.917301595211029, mat0 norm 15.551688194274902\n",
      "Epoch 489, loss 0.9171461462974548, mat0 norm 15.550007820129395\n",
      "Epoch 490, loss 0.9169905185699463, mat0 norm 15.548310279846191\n",
      "Epoch 491, loss 0.916835367679596, mat0 norm 15.546618461608887\n",
      "Epoch 492, loss 0.9166803956031799, mat0 norm 15.54493236541748\n",
      "Epoch 493, loss 0.9165253639221191, mat0 norm 15.543234825134277\n",
      "Epoch 494, loss 0.9163705110549927, mat0 norm 15.541539192199707\n",
      "Epoch 495, loss 0.9162161350250244, mat0 norm 15.539847373962402\n",
      "Epoch 496, loss 0.9160616397857666, mat0 norm 15.538166046142578\n",
      "Epoch 497, loss 0.9159073829650879, mat0 norm 15.53647232055664\n",
      "Epoch 498, loss 0.9157533049583435, mat0 norm 15.534783363342285\n",
      "Epoch 499, loss 0.9155995845794678, mat0 norm 15.533101081848145\n",
      "Epoch 500, loss 0.9154455661773682, mat0 norm 15.531404495239258\n",
      "Epoch 501, loss 0.9152917861938477, mat0 norm 15.529711723327637\n",
      "Epoch 502, loss 0.9151387810707092, mat0 norm 15.528037071228027\n",
      "Epoch 503, loss 0.9149852991104126, mat0 norm 15.52634334564209\n",
      "Epoch 504, loss 0.9148321151733398, mat0 norm 15.524654388427734\n",
      "Epoch 505, loss 0.9146795272827148, mat0 norm 15.522966384887695\n",
      "Epoch 506, loss 0.9145265221595764, mat0 norm 15.521286964416504\n",
      "Epoch 507, loss 0.9143737554550171, mat0 norm 15.519598960876465\n",
      "Epoch 508, loss 0.9142214059829712, mat0 norm 15.517908096313477\n",
      "Epoch 509, loss 0.9140692949295044, mat0 norm 15.516237258911133\n",
      "Epoch 510, loss 0.9139170050621033, mat0 norm 15.514546394348145\n",
      "Epoch 511, loss 0.9137649536132812, mat0 norm 15.512857437133789\n",
      "Epoch 512, loss 0.9136133193969727, mat0 norm 15.511181831359863\n",
      "Epoch 513, loss 0.9134613871574402, mat0 norm 15.509493827819824\n",
      "Epoch 514, loss 0.9133099317550659, mat0 norm 15.507808685302734\n",
      "Epoch 515, loss 0.9131587743759155, mat0 norm 15.506120681762695\n",
      "Epoch 516, loss 0.9130074977874756, mat0 norm 15.504448890686035\n",
      "Epoch 517, loss 0.9128563404083252, mat0 norm 15.502766609191895\n",
      "Epoch 518, loss 0.912705659866333, mat0 norm 15.501078605651855\n",
      "Epoch 519, loss 0.9125549793243408, mat0 norm 15.499409675598145\n",
      "Epoch 520, loss 0.9124043583869934, mat0 norm 15.497725486755371\n",
      "Epoch 521, loss 0.9122538566589355, mat0 norm 15.496037483215332\n",
      "Epoch 522, loss 0.9121037721633911, mat0 norm 15.494369506835938\n",
      "Epoch 523, loss 0.9119535684585571, mat0 norm 15.492687225341797\n",
      "Epoch 524, loss 0.9118035435676575, mat0 norm 15.491002082824707\n",
      "Epoch 525, loss 0.9116540551185608, mat0 norm 15.489333152770996\n",
      "Epoch 526, loss 0.9115043878555298, mat0 norm 15.487656593322754\n",
      "Epoch 527, loss 0.9113547801971436, mat0 norm 15.485971450805664\n",
      "Epoch 528, loss 0.9112056493759155, mat0 norm 15.484289169311523\n",
      "Epoch 529, loss 0.9110564589500427, mat0 norm 15.482623100280762\n",
      "Epoch 530, loss 0.9109072685241699, mat0 norm 15.480937957763672\n",
      "Epoch 531, loss 0.9107586741447449, mat0 norm 15.47926139831543\n",
      "Epoch 532, loss 0.9106099605560303, mat0 norm 15.477592468261719\n",
      "Epoch 533, loss 0.9104613065719604, mat0 norm 15.47591495513916\n",
      "Epoch 534, loss 0.9103128910064697, mat0 norm 15.47423267364502\n",
      "Epoch 535, loss 0.9101648926734924, mat0 norm 15.472575187683105\n",
      "Epoch 536, loss 0.9100165963172913, mat0 norm 15.470890998840332\n",
      "Epoch 537, loss 0.909868597984314, mat0 norm 15.46921443939209\n",
      "Epoch 538, loss 0.9097210168838501, mat0 norm 15.467550277709961\n",
      "Epoch 539, loss 0.9095733761787415, mat0 norm 15.465873718261719\n",
      "Epoch 540, loss 0.9094257354736328, mat0 norm 15.464195251464844\n",
      "Epoch 541, loss 0.9092787504196167, mat0 norm 15.462518692016602\n",
      "Epoch 542, loss 0.9091314673423767, mat0 norm 15.460856437683105\n",
      "Epoch 543, loss 0.9089843034744263, mat0 norm 15.459182739257812\n",
      "Epoch 544, loss 0.908837616443634, mat0 norm 15.457507133483887\n",
      "Epoch 545, loss 0.9086909294128418, mat0 norm 15.455848693847656\n",
      "Epoch 546, loss 0.9085440635681152, mat0 norm 15.454168319702148\n",
      "Epoch 547, loss 0.9083978533744812, mat0 norm 15.452496528625488\n",
      "Epoch 548, loss 0.9082516431808472, mat0 norm 15.45083999633789\n",
      "Epoch 549, loss 0.9081053137779236, mat0 norm 15.4491605758667\n",
      "Epoch 550, loss 0.9079593420028687, mat0 norm 15.447489738464355\n",
      "Epoch 551, loss 0.9078136682510376, mat0 norm 15.445830345153809\n",
      "Epoch 552, loss 0.9076679944992065, mat0 norm 15.444157600402832\n",
      "Epoch 553, loss 0.9075223207473755, mat0 norm 15.442487716674805\n",
      "Epoch 554, loss 0.9073772430419922, mat0 norm 15.440831184387207\n",
      "Epoch 555, loss 0.9072317481040955, mat0 norm 15.439159393310547\n",
      "Epoch 556, loss 0.9070864915847778, mat0 norm 15.437483787536621\n",
      "Epoch 557, loss 0.906941831111908, mat0 norm 15.435829162597656\n",
      "Epoch 558, loss 0.906796932220459, mat0 norm 15.434161186218262\n",
      "Epoch 559, loss 0.90665203332901, mat0 norm 15.432491302490234\n",
      "Epoch 560, loss 0.9065078496932983, mat0 norm 15.430822372436523\n",
      "Epoch 561, loss 0.9063633680343628, mat0 norm 15.429167747497559\n",
      "Epoch 562, loss 0.9062190651893616, mat0 norm 15.427498817443848\n",
      "Epoch 563, loss 0.9060751795768738, mat0 norm 15.42583179473877\n",
      "Epoch 564, loss 0.9059311747550964, mat0 norm 15.424176216125488\n",
      "Epoch 565, loss 0.9057872295379639, mat0 norm 15.422510147094727\n",
      "Epoch 566, loss 0.9056437015533447, mat0 norm 15.420842170715332\n",
      "Epoch 567, loss 0.9055002331733704, mat0 norm 15.419193267822266\n",
      "Epoch 568, loss 0.9053567051887512, mat0 norm 15.417523384094238\n",
      "Epoch 569, loss 0.9052137136459351, mat0 norm 15.41585636138916\n",
      "Epoch 570, loss 0.9050706624984741, mat0 norm 15.414205551147461\n",
      "Epoch 571, loss 0.9049275517463684, mat0 norm 15.4125394821167\n",
      "Epoch 572, loss 0.9047847986221313, mat0 norm 15.410880088806152\n",
      "Epoch 573, loss 0.9046422839164734, mat0 norm 15.409226417541504\n",
      "Epoch 574, loss 0.9044997096061707, mat0 norm 15.407567024230957\n",
      "Epoch 575, loss 0.9043571949005127, mat0 norm 15.40589714050293\n",
      "Epoch 576, loss 0.9042151570320129, mat0 norm 15.40424919128418\n",
      "Epoch 577, loss 0.9040729403495789, mat0 norm 15.402585983276367\n",
      "Epoch 578, loss 0.9039309024810791, mat0 norm 15.400924682617188\n",
      "Epoch 579, loss 0.9037893414497375, mat0 norm 15.39927864074707\n",
      "Epoch 580, loss 0.9036474823951721, mat0 norm 15.397612571716309\n",
      "Epoch 581, loss 0.903505802154541, mat0 norm 15.395949363708496\n",
      "Epoch 582, loss 0.9033647775650024, mat0 norm 15.394307136535645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 583, loss 0.9032233357429504, mat0 norm 15.392643928527832\n",
      "Epoch 584, loss 0.9030820727348328, mat0 norm 15.390986442565918\n",
      "Epoch 585, loss 0.9029414653778076, mat0 norm 15.389336585998535\n",
      "Epoch 586, loss 0.902800440788269, mat0 norm 15.387677192687988\n",
      "Epoch 587, loss 0.90265953540802, mat0 norm 15.386018753051758\n",
      "Epoch 588, loss 0.9025192856788635, mat0 norm 15.384360313415527\n",
      "Epoch 589, loss 0.9023787975311279, mat0 norm 15.38271713256836\n",
      "Epoch 590, loss 0.9022383093833923, mat0 norm 15.381056785583496\n",
      "Epoch 591, loss 0.9020984172821045, mat0 norm 15.379402160644531\n",
      "Epoch 592, loss 0.9019582867622375, mat0 norm 15.377756118774414\n",
      "Epoch 593, loss 0.9018181562423706, mat0 norm 15.376096725463867\n",
      "Epoch 594, loss 0.9016786813735962, mat0 norm 15.374441146850586\n",
      "Epoch 595, loss 0.9015389680862427, mat0 norm 15.372797966003418\n",
      "Epoch 596, loss 0.9013993740081787, mat0 norm 15.371145248413086\n",
      "Epoch 597, loss 0.901260256767273, mat0 norm 15.369489669799805\n",
      "Epoch 598, loss 0.9011210203170776, mat0 norm 15.36784553527832\n",
      "Epoch 599, loss 0.9009817242622375, mat0 norm 15.366191864013672\n",
      "Epoch 600, loss 0.9008429646492004, mat0 norm 15.364534378051758\n",
      "Epoch 601, loss 0.9007042050361633, mat0 norm 15.362902641296387\n",
      "Epoch 602, loss 0.9005653858184814, mat0 norm 15.361247062683105\n",
      "Epoch 603, loss 0.900426983833313, mat0 norm 15.359588623046875\n",
      "Epoch 604, loss 0.9002885818481445, mat0 norm 15.357952117919922\n",
      "Epoch 605, loss 0.9001500606536865, mat0 norm 15.35629940032959\n",
      "Epoch 606, loss 0.9000121355056763, mat0 norm 15.35464859008789\n",
      "Epoch 607, loss 0.899874210357666, mat0 norm 15.353010177612305\n",
      "Epoch 608, loss 0.8997361063957214, mat0 norm 15.351359367370605\n",
      "Epoch 609, loss 0.8995984792709351, mat0 norm 15.349702835083008\n",
      "Epoch 610, loss 0.8994607925415039, mat0 norm 15.348066329956055\n",
      "Epoch 611, loss 0.8993232250213623, mat0 norm 15.346419334411621\n",
      "Epoch 612, loss 0.8991860151290894, mat0 norm 15.344765663146973\n",
      "Epoch 613, loss 0.8990488052368164, mat0 norm 15.343132019042969\n",
      "Epoch 614, loss 0.8989115357398987, mat0 norm 15.341483116149902\n",
      "Epoch 615, loss 0.8987746238708496, mat0 norm 15.339831352233887\n",
      "Epoch 616, loss 0.8986377716064453, mat0 norm 15.33819580078125\n",
      "Epoch 617, loss 0.898500919342041, mat0 norm 15.33654499053955\n",
      "Epoch 618, loss 0.8983644843101501, mat0 norm 15.334897994995117\n",
      "Epoch 619, loss 0.898228108882904, mat0 norm 15.33326530456543\n",
      "Epoch 620, loss 0.8980916142463684, mat0 norm 15.331619262695312\n",
      "Epoch 621, loss 0.8979555368423462, mat0 norm 15.329965591430664\n",
      "Epoch 622, loss 0.897819459438324, mat0 norm 15.328337669372559\n",
      "Epoch 623, loss 0.897683322429657, mat0 norm 15.326689720153809\n",
      "Epoch 624, loss 0.8975477814674377, mat0 norm 15.325044631958008\n",
      "Epoch 625, loss 0.8974120020866394, mat0 norm 15.323413848876953\n",
      "Epoch 626, loss 0.8972762823104858, mat0 norm 15.32176685333252\n",
      "Epoch 627, loss 0.8971410393714905, mat0 norm 15.320121765136719\n",
      "Epoch 628, loss 0.8970056772232056, mat0 norm 15.318492889404297\n",
      "Epoch 629, loss 0.8968703150749207, mat0 norm 15.316850662231445\n",
      "Epoch 630, loss 0.8967355489730835, mat0 norm 15.315202713012695\n",
      "Epoch 631, loss 0.8966005444526672, mat0 norm 15.313570976257324\n",
      "Epoch 632, loss 0.8964655995368958, mat0 norm 15.311931610107422\n",
      "Epoch 633, loss 0.8963311910629272, mat0 norm 15.310288429260254\n",
      "Epoch 634, loss 0.8961965441703796, mat0 norm 15.30865478515625\n",
      "Epoch 635, loss 0.896061897277832, mat0 norm 15.307012557983398\n",
      "Epoch 636, loss 0.8959278464317322, mat0 norm 15.305371284484863\n",
      "Epoch 637, loss 0.895793616771698, mat0 norm 15.303750038146973\n",
      "Epoch 638, loss 0.8956592679023743, mat0 norm 15.302102088928223\n",
      "Epoch 639, loss 0.8955256342887878, mat0 norm 15.30046558380127\n",
      "Epoch 640, loss 0.8953916430473328, mat0 norm 15.298833847045898\n",
      "Epoch 641, loss 0.895257830619812, mat0 norm 15.297198295593262\n",
      "Epoch 642, loss 0.8951245546340942, mat0 norm 15.295554161071777\n",
      "Epoch 643, loss 0.8949909806251526, mat0 norm 15.293932914733887\n",
      "Epoch 644, loss 0.8948575258255005, mat0 norm 15.292291641235352\n",
      "Epoch 645, loss 0.8947246670722961, mat0 norm 15.290653228759766\n",
      "Epoch 646, loss 0.8945913910865784, mat0 norm 15.28902816772461\n",
      "Epoch 647, loss 0.8944582939147949, mat0 norm 15.287392616271973\n",
      "Epoch 648, loss 0.8943257331848145, mat0 norm 15.285767555236816\n",
      "Epoch 649, loss 0.8941928148269653, mat0 norm 15.284130096435547\n",
      "Epoch 650, loss 0.8940600156784058, mat0 norm 15.282490730285645\n",
      "Epoch 651, loss 0.8939279317855835, mat0 norm 15.280867576599121\n",
      "Epoch 652, loss 0.8937954902648926, mat0 norm 15.279231071472168\n",
      "Epoch 653, loss 0.8936631083488464, mat0 norm 15.277592658996582\n",
      "Epoch 654, loss 0.8935312032699585, mat0 norm 15.275970458984375\n",
      "Epoch 655, loss 0.8933990001678467, mat0 norm 15.274338722229004\n",
      "Epoch 656, loss 0.8932671546936035, mat0 norm 15.27270221710205\n",
      "Epoch 657, loss 0.8931355476379395, mat0 norm 15.271081924438477\n",
      "Epoch 658, loss 0.8930037617683411, mat0 norm 15.269447326660156\n",
      "Epoch 659, loss 0.8928722143173218, mat0 norm 15.26781177520752\n",
      "Epoch 660, loss 0.8927409648895264, mat0 norm 15.266189575195312\n",
      "Epoch 661, loss 0.8926094770431519, mat0 norm 15.264558792114258\n",
      "Epoch 662, loss 0.8924784660339355, mat0 norm 15.262924194335938\n",
      "Epoch 663, loss 0.8923473358154297, mat0 norm 15.26130199432373\n",
      "Epoch 664, loss 0.8922163248062134, mat0 norm 15.259676933288574\n",
      "Epoch 665, loss 0.892085611820221, mat0 norm 15.258039474487305\n",
      "Epoch 666, loss 0.8919549584388733, mat0 norm 15.25642204284668\n",
      "Epoch 667, loss 0.8918242454528809, mat0 norm 15.25479507446289\n",
      "Epoch 668, loss 0.8916939496994019, mat0 norm 15.25316047668457\n",
      "Epoch 669, loss 0.8915635943412781, mat0 norm 15.251546859741211\n",
      "Epoch 670, loss 0.8914331197738647, mat0 norm 15.249917030334473\n",
      "Epoch 671, loss 0.8913033604621887, mat0 norm 15.248286247253418\n",
      "Epoch 672, loss 0.8911731839179993, mat0 norm 15.24666690826416\n",
      "Epoch 673, loss 0.8910430669784546, mat0 norm 15.245037078857422\n",
      "Epoch 674, loss 0.8909136652946472, mat0 norm 15.243424415588379\n",
      "Epoch 675, loss 0.8907837867736816, mat0 norm 15.24179458618164\n",
      "Epoch 676, loss 0.8906542062759399, mat0 norm 15.240166664123535\n",
      "Epoch 677, loss 0.8905249834060669, mat0 norm 15.238550186157227\n",
      "Epoch 678, loss 0.89039546251297, mat0 norm 15.236920356750488\n",
      "Epoch 679, loss 0.8902662992477417, mat0 norm 15.23529052734375\n",
      "Epoch 680, loss 0.8901373744010925, mat0 norm 15.23367977142334\n",
      "Epoch 681, loss 0.8900082111358643, mat0 norm 15.232050895690918\n",
      "Epoch 682, loss 0.8898793458938599, mat0 norm 15.230428695678711\n",
      "Epoch 683, loss 0.8897506594657898, mat0 norm 15.228812217712402\n",
      "Epoch 684, loss 0.8896219730377197, mat0 norm 15.227187156677246\n",
      "Epoch 685, loss 0.8894935846328735, mat0 norm 15.22556209564209\n",
      "Epoch 686, loss 0.8893651366233826, mat0 norm 15.223952293395996\n",
      "Epoch 687, loss 0.8892366886138916, mat0 norm 15.222328186035156\n",
      "Epoch 688, loss 0.8891086578369141, mat0 norm 15.220703125\n",
      "Epoch 689, loss 0.8889805674552917, mat0 norm 15.219094276428223\n",
      "Epoch 690, loss 0.8888522386550903, mat0 norm 15.217462539672852\n",
      "Epoch 691, loss 0.8887248635292053, mat0 norm 15.215843200683594\n",
      "Epoch 692, loss 0.8885968923568726, mat0 norm 15.214232444763184\n",
      "Epoch 693, loss 0.8884692192077637, mat0 norm 15.21261215209961\n",
      "Epoch 694, loss 0.8883419036865234, mat0 norm 15.211002349853516\n",
      "Epoch 695, loss 0.8882150650024414, mat0 norm 15.209457397460938\n",
      "Epoch 696, loss 0.8880902528762817, mat0 norm 15.20796012878418\n",
      "Epoch 697, loss 0.887965738773346, mat0 norm 15.206476211547852\n",
      "Epoch 698, loss 0.887840986251831, mat0 norm 15.204978942871094\n",
      "Epoch 699, loss 0.8877166509628296, mat0 norm 15.203485488891602\n",
      "Epoch 700, loss 0.8875923752784729, mat0 norm 15.202003479003906\n",
      "Epoch 701, loss 0.8874678611755371, mat0 norm 15.200509071350098\n",
      "Epoch 702, loss 0.8873438835144043, mat0 norm 15.199013710021973\n",
      "Epoch 703, loss 0.8872197866439819, mat0 norm 15.19753360748291\n",
      "Epoch 704, loss 0.8870958089828491, mat0 norm 15.196040153503418\n",
      "Epoch 705, loss 0.8869721293449402, mat0 norm 15.194540977478027\n",
      "Epoch 706, loss 0.8868482708930969, mat0 norm 15.193062782287598\n",
      "Epoch 707, loss 0.8867244720458984, mat0 norm 15.191570281982422\n",
      "Epoch 708, loss 0.8866012692451477, mat0 norm 15.190093994140625\n",
      "Epoch 709, loss 0.8864776492118835, mat0 norm 15.188599586486816\n",
      "Epoch 710, loss 0.8863544464111328, mat0 norm 15.187105178833008\n",
      "Epoch 711, loss 0.8862314224243164, mat0 norm 15.185626983642578\n",
      "Epoch 712, loss 0.8861080408096313, mat0 norm 15.184137344360352\n",
      "Epoch 713, loss 0.885985255241394, mat0 norm 15.182646751403809\n",
      "Epoch 714, loss 0.8858623504638672, mat0 norm 15.181163787841797\n",
      "Epoch 715, loss 0.8857393860816956, mat0 norm 15.179677963256836\n",
      "Epoch 716, loss 0.8856168985366821, mat0 norm 15.178183555603027\n",
      "Epoch 717, loss 0.8854942321777344, mat0 norm 15.17670726776123\n",
      "Epoch 718, loss 0.8853716254234314, mat0 norm 15.175215721130371\n",
      "Epoch 719, loss 0.8852496147155762, mat0 norm 15.173727035522461\n",
      "Epoch 720, loss 0.8851271867752075, mat0 norm 15.17225170135498\n",
      "Epoch 721, loss 0.8850048780441284, mat0 norm 15.17076301574707\n",
      "Epoch 722, loss 0.8848829865455627, mat0 norm 15.169282913208008\n",
      "Epoch 723, loss 0.884760856628418, mat0 norm 15.167794227600098\n",
      "Epoch 724, loss 0.8846392035484314, mat0 norm 15.166311264038086\n",
      "Epoch 725, loss 0.8845174312591553, mat0 norm 15.164834022521973\n",
      "Epoch 726, loss 0.8843955993652344, mat0 norm 15.163344383239746\n",
      "Epoch 727, loss 0.8842741847038269, mat0 norm 15.161853790283203\n",
      "Epoch 728, loss 0.8841526508331299, mat0 norm 15.160378456115723\n",
      "Epoch 729, loss 0.8840311765670776, mat0 norm 15.158892631530762\n",
      "Epoch 730, loss 0.8839102387428284, mat0 norm 15.1574068069458\n",
      "Epoch 731, loss 0.8837888836860657, mat0 norm 15.155929565429688\n",
      "Epoch 732, loss 0.8836678266525269, mat0 norm 15.154449462890625\n",
      "Epoch 733, loss 0.8835471272468567, mat0 norm 15.152975082397461\n",
      "Epoch 734, loss 0.8834259510040283, mat0 norm 15.15148639678955\n",
      "Epoch 735, loss 0.8833053708076477, mat0 norm 15.150003433227539\n",
      "Epoch 736, loss 0.8831847310066223, mat0 norm 15.148528099060059\n",
      "Epoch 737, loss 0.8830639719963074, mat0 norm 15.14704704284668\n",
      "Epoch 738, loss 0.8829438090324402, mat0 norm 15.145559310913086\n",
      "Epoch 739, loss 0.8828233480453491, mat0 norm 15.144084930419922\n",
      "Epoch 740, loss 0.8827028870582581, mat0 norm 15.142603874206543\n",
      "Epoch 741, loss 0.8825830817222595, mat0 norm 15.141119003295898\n",
      "Epoch 742, loss 0.8824628591537476, mat0 norm 15.139647483825684\n",
      "Epoch 743, loss 0.8823428153991699, mat0 norm 15.138163566589355\n",
      "Epoch 744, loss 0.8822231292724609, mat0 norm 15.136690139770508\n",
      "Epoch 745, loss 0.8821032643318176, mat0 norm 15.135210037231445\n",
      "Epoch 746, loss 0.8819836974143982, mat0 norm 15.1337308883667\n",
      "Epoch 747, loss 0.8818641901016235, mat0 norm 15.132261276245117\n",
      "Epoch 748, loss 0.8817479610443115, mat0 norm 15.130779266357422\n",
      "Epoch 749, loss 0.881632924079895, mat0 norm 15.129325866699219\n",
      "Epoch 750, loss 0.8815177083015442, mat0 norm 15.127889633178711\n",
      "Epoch 751, loss 0.881402313709259, mat0 norm 15.126429557800293\n",
      "Epoch 752, loss 0.881287693977356, mat0 norm 15.12499713897705\n",
      "Epoch 753, loss 0.8811725378036499, mat0 norm 15.123543739318848\n",
      "Epoch 754, loss 0.8810575604438782, mat0 norm 15.122093200683594\n",
      "Epoch 755, loss 0.8809430599212646, mat0 norm 15.120658874511719\n",
      "Epoch 756, loss 0.8808282017707825, mat0 norm 15.11921215057373\n",
      "Epoch 757, loss 0.8807138204574585, mat0 norm 15.117756843566895\n",
      "Epoch 758, loss 0.880599319934845, mat0 norm 15.11633014678955\n",
      "Epoch 759, loss 0.8804845213890076, mat0 norm 15.114874839782715\n",
      "Epoch 760, loss 0.8803706765174866, mat0 norm 15.113426208496094\n",
      "Epoch 761, loss 0.8802562355995178, mat0 norm 15.111993789672852\n",
      "Epoch 762, loss 0.8801420331001282, mat0 norm 15.11054515838623\n",
      "Epoch 763, loss 0.8800281286239624, mat0 norm 15.109107971191406\n",
      "Epoch 764, loss 0.8799139261245728, mat0 norm 15.107659339904785\n",
      "Epoch 765, loss 0.8798002004623413, mat0 norm 15.106218338012695\n",
      "Epoch 766, loss 0.8796864151954651, mat0 norm 15.104781150817871\n",
      "Epoch 767, loss 0.8795724511146545, mat0 norm 15.103336334228516\n",
      "Epoch 768, loss 0.879459023475647, mat0 norm 15.101889610290527\n",
      "Epoch 769, loss 0.8793454170227051, mat0 norm 15.100455284118652\n",
      "Epoch 770, loss 0.8792318105697632, mat0 norm 15.09900951385498\n",
      "Epoch 771, loss 0.8791185617446899, mat0 norm 15.097572326660156\n",
      "Epoch 772, loss 0.8790050745010376, mat0 norm 15.096131324768066\n",
      "Epoch 773, loss 0.8788920640945435, mat0 norm 15.094682693481445\n",
      "Epoch 774, loss 0.8787790536880493, mat0 norm 15.093252182006836\n",
      "Epoch 775, loss 0.878665566444397, mat0 norm 15.091800689697266\n",
      "Epoch 776, loss 0.8785529136657715, mat0 norm 15.090357780456543\n",
      "Epoch 777, loss 0.8784398436546326, mat0 norm 15.088930130004883\n",
      "Epoch 778, loss 0.8783270120620728, mat0 norm 15.087485313415527\n",
      "Epoch 779, loss 0.8782145977020264, mat0 norm 15.086052894592285\n",
      "Epoch 780, loss 0.8781017065048218, mat0 norm 15.084609031677246\n",
      "Epoch 781, loss 0.8779892325401306, mat0 norm 15.083166122436523\n",
      "Epoch 782, loss 0.8778768181800842, mat0 norm 15.081733703613281\n",
      "Epoch 783, loss 0.8777642250061035, mat0 norm 15.080294609069824\n",
      "Epoch 784, loss 0.8776522278785706, mat0 norm 15.07884693145752\n",
      "Epoch 785, loss 0.8775397539138794, mat0 norm 15.07741928100586\n",
      "Epoch 786, loss 0.8774274587631226, mat0 norm 15.075973510742188\n",
      "Epoch 787, loss 0.8773157000541687, mat0 norm 15.074543952941895\n",
      "Epoch 788, loss 0.8772034049034119, mat0 norm 15.073103904724121\n",
      "Epoch 789, loss 0.877091646194458, mat0 norm 15.071662902832031\n",
      "Epoch 790, loss 0.8769798874855042, mat0 norm 15.070234298706055\n",
      "Epoch 791, loss 0.8768678903579712, mat0 norm 15.068796157836914\n",
      "Epoch 792, loss 0.8767566084861755, mat0 norm 15.067352294921875\n",
      "Epoch 793, loss 0.8766448497772217, mat0 norm 15.065924644470215\n",
      "Epoch 794, loss 0.8765332102775574, mat0 norm 15.064483642578125\n",
      "Epoch 795, loss 0.876422107219696, mat0 norm 15.063058853149414\n",
      "Epoch 796, loss 0.8763104677200317, mat0 norm 15.061614990234375\n",
      "Epoch 797, loss 0.8761993050575256, mat0 norm 15.060175895690918\n",
      "Epoch 798, loss 0.8760881423950195, mat0 norm 15.058747291564941\n",
      "Epoch 799, loss 0.8759768009185791, mat0 norm 15.057310104370117\n",
      "Epoch 800, loss 0.8758662343025208, mat0 norm 15.055872917175293\n",
      "Epoch 801, loss 0.8757550120353699, mat0 norm 15.054445266723633\n",
      "Epoch 802, loss 0.8756441473960876, mat0 norm 15.05300521850586\n",
      "Epoch 803, loss 0.8755335211753845, mat0 norm 15.051580429077148\n",
      "Epoch 804, loss 0.8754225373268127, mat0 norm 15.05014419555664\n",
      "Epoch 805, loss 0.8753121495246887, mat0 norm 15.048700332641602\n",
      "Epoch 806, loss 0.8752016425132751, mat0 norm 15.047281265258789\n",
      "Epoch 807, loss 0.8750908374786377, mat0 norm 15.045835494995117\n",
      "Epoch 808, loss 0.8749808669090271, mat0 norm 15.044416427612305\n",
      "Epoch 809, loss 0.8748703002929688, mat0 norm 15.04297924041748\n",
      "Epoch 810, loss 0.8747601509094238, mat0 norm 15.04154109954834\n",
      "Epoch 811, loss 0.8746501207351685, mat0 norm 15.040120124816895\n",
      "Epoch 812, loss 0.8745397329330444, mat0 norm 15.038680076599121\n",
      "Epoch 813, loss 0.8744300603866577, mat0 norm 15.037247657775879\n",
      "Epoch 814, loss 0.8743199110031128, mat0 norm 15.035822868347168\n",
      "Epoch 815, loss 0.874210000038147, mat0 norm 15.034388542175293\n",
      "Epoch 816, loss 0.8741003274917603, mat0 norm 15.032962799072266\n",
      "Epoch 817, loss 0.8739904165267944, mat0 norm 15.031527519226074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 818, loss 0.8738810420036316, mat0 norm 15.030091285705566\n",
      "Epoch 819, loss 0.8737714886665344, mat0 norm 15.028674125671387\n",
      "Epoch 820, loss 0.8736618161201477, mat0 norm 15.027236938476562\n",
      "Epoch 821, loss 0.8735527992248535, mat0 norm 15.025815963745117\n",
      "Epoch 822, loss 0.8734432458877563, mat0 norm 15.024380683898926\n",
      "Epoch 823, loss 0.8733426332473755, mat0 norm 15.02294635772705\n",
      "Epoch 824, loss 0.8732469081878662, mat0 norm 15.021590232849121\n",
      "Epoch 825, loss 0.8731508255004883, mat0 norm 15.02021312713623\n",
      "Epoch 826, loss 0.8730553984642029, mat0 norm 15.018838882446289\n",
      "Epoch 827, loss 0.8729596734046936, mat0 norm 15.017478942871094\n",
      "Epoch 828, loss 0.8728640079498291, mat0 norm 15.016105651855469\n",
      "Epoch 829, loss 0.872768759727478, mat0 norm 15.014748573303223\n",
      "Epoch 830, loss 0.8726729154586792, mat0 norm 15.013374328613281\n",
      "Epoch 831, loss 0.8725776672363281, mat0 norm 15.01200008392334\n",
      "Epoch 832, loss 0.8724824786186218, mat0 norm 15.010647773742676\n",
      "Epoch 833, loss 0.8723869323730469, mat0 norm 15.009269714355469\n",
      "Epoch 834, loss 0.8722921013832092, mat0 norm 15.007911682128906\n",
      "Epoch 835, loss 0.8721967935562134, mat0 norm 15.006545066833496\n",
      "Epoch 836, loss 0.872101902961731, mat0 norm 15.005173683166504\n",
      "Epoch 837, loss 0.872006893157959, mat0 norm 15.003814697265625\n",
      "Epoch 838, loss 0.8719117045402527, mat0 norm 15.00244140625\n",
      "Epoch 839, loss 0.8718173503875732, mat0 norm 15.001073837280273\n",
      "Epoch 840, loss 0.871722400188446, mat0 norm 14.99971866607666\n",
      "Epoch 841, loss 0.8716276288032532, mat0 norm 14.998346328735352\n",
      "Epoch 842, loss 0.8715331554412842, mat0 norm 14.996990203857422\n",
      "Epoch 843, loss 0.8714383244514465, mat0 norm 14.99561882019043\n",
      "Epoch 844, loss 0.8713440895080566, mat0 norm 14.99424934387207\n",
      "Epoch 845, loss 0.8712494969367981, mat0 norm 14.992894172668457\n",
      "Epoch 846, loss 0.8711550235748291, mat0 norm 14.991524696350098\n",
      "Epoch 847, loss 0.8710609674453735, mat0 norm 14.990172386169434\n",
      "Epoch 848, loss 0.8709665536880493, mat0 norm 14.988801002502441\n",
      "Epoch 849, loss 0.8708724975585938, mat0 norm 14.987432479858398\n",
      "Epoch 850, loss 0.8707783818244934, mat0 norm 14.98608112335205\n",
      "Epoch 851, loss 0.8706841468811035, mat0 norm 14.984711647033691\n",
      "Epoch 852, loss 0.8705904483795166, mat0 norm 14.983352661132812\n",
      "Epoch 853, loss 0.8704962730407715, mat0 norm 14.981990814208984\n",
      "Epoch 854, loss 0.8704025745391846, mat0 norm 14.980624198913574\n",
      "Epoch 855, loss 0.8703087568283081, mat0 norm 14.979265213012695\n",
      "Epoch 856, loss 0.8702148199081421, mat0 norm 14.977901458740234\n",
      "Epoch 857, loss 0.8701215982437134, mat0 norm 14.976533889770508\n",
      "Epoch 858, loss 0.8700277209281921, mat0 norm 14.975177764892578\n",
      "Epoch 859, loss 0.8699342012405396, mat0 norm 14.973812103271484\n",
      "Epoch 860, loss 0.8698408603668213, mat0 norm 14.972463607788086\n",
      "Epoch 861, loss 0.8697471618652344, mat0 norm 14.971095085144043\n",
      "Epoch 862, loss 0.8696542978286743, mat0 norm 14.969727516174316\n",
      "Epoch 863, loss 0.8695617914199829, mat0 norm 14.968489646911621\n",
      "Epoch 864, loss 0.8694711923599243, mat0 norm 14.96725082397461\n",
      "Epoch 865, loss 0.8693807125091553, mat0 norm 14.966025352478027\n",
      "Epoch 866, loss 0.8692898154258728, mat0 norm 14.964783668518066\n",
      "Epoch 867, loss 0.8691996335983276, mat0 norm 14.963547706604004\n",
      "Epoch 868, loss 0.869109034538269, mat0 norm 14.962325096130371\n",
      "Epoch 869, loss 0.8690186142921448, mat0 norm 14.961082458496094\n",
      "Epoch 870, loss 0.8689284324645996, mat0 norm 14.95986270904541\n",
      "Epoch 871, loss 0.8688379526138306, mat0 norm 14.958622932434082\n",
      "Epoch 872, loss 0.8687480688095093, mat0 norm 14.957387924194336\n",
      "Epoch 873, loss 0.8686577081680298, mat0 norm 14.956161499023438\n",
      "Epoch 874, loss 0.8685675859451294, mat0 norm 14.954928398132324\n",
      "Epoch 875, loss 0.8684778809547424, mat0 norm 14.953702926635742\n",
      "Epoch 876, loss 0.8683876395225525, mat0 norm 14.952463150024414\n",
      "Epoch 877, loss 0.8682979345321655, mat0 norm 14.951226234436035\n",
      "Epoch 878, loss 0.8682079911231995, mat0 norm 14.950002670288086\n",
      "Epoch 879, loss 0.8681181073188782, mat0 norm 14.948768615722656\n",
      "Epoch 880, loss 0.8680287599563599, mat0 norm 14.947545051574707\n",
      "Epoch 881, loss 0.867938756942749, mat0 norm 14.946308135986328\n",
      "Epoch 882, loss 0.8678493499755859, mat0 norm 14.945072174072266\n",
      "Epoch 883, loss 0.8677597045898438, mat0 norm 14.943845748901367\n",
      "Epoch 884, loss 0.8676700592041016, mat0 norm 14.942609786987305\n",
      "Epoch 885, loss 0.8675810694694519, mat0 norm 14.941390037536621\n",
      "Epoch 886, loss 0.8674912452697754, mat0 norm 14.940152168273926\n",
      "Epoch 887, loss 0.8674022555351257, mat0 norm 14.938921928405762\n",
      "Epoch 888, loss 0.867313027381897, mat0 norm 14.937698364257812\n",
      "Epoch 889, loss 0.8672235012054443, mat0 norm 14.936460494995117\n",
      "Epoch 890, loss 0.8671348094940186, mat0 norm 14.935242652893066\n",
      "Epoch 891, loss 0.8670454621315002, mat0 norm 14.934009552001953\n",
      "Epoch 892, loss 0.8669565320014954, mat0 norm 14.932770729064941\n",
      "Epoch 893, loss 0.8668676018714905, mat0 norm 14.931549072265625\n",
      "Epoch 894, loss 0.8667784333229065, mat0 norm 14.930314064025879\n",
      "Epoch 895, loss 0.8666900992393494, mat0 norm 14.929096221923828\n",
      "Epoch 896, loss 0.8666009902954102, mat0 norm 14.927861213684082\n",
      "Epoch 897, loss 0.8665124177932739, mat0 norm 14.926623344421387\n",
      "Epoch 898, loss 0.8664237260818481, mat0 norm 14.925405502319336\n",
      "Epoch 899, loss 0.866334855556488, mat0 norm 14.924169540405273\n",
      "Epoch 900, loss 0.8662467002868652, mat0 norm 14.922953605651855\n",
      "Epoch 901, loss 0.8661578893661499, mat0 norm 14.921720504760742\n",
      "Epoch 902, loss 0.8660696148872375, mat0 norm 14.920485496520996\n",
      "Epoch 903, loss 0.8659813404083252, mat0 norm 14.919267654418945\n",
      "Epoch 904, loss 0.8658927083015442, mat0 norm 14.918034553527832\n",
      "Epoch 905, loss 0.8658047914505005, mat0 norm 14.9168119430542\n",
      "Epoch 906, loss 0.8657163381576538, mat0 norm 14.9155855178833\n",
      "Epoch 907, loss 0.8656283617019653, mat0 norm 14.914349555969238\n",
      "Epoch 908, loss 0.8655401468276978, mat0 norm 14.913130760192871\n",
      "Epoch 909, loss 0.8654519319534302, mat0 norm 14.911894798278809\n",
      "Epoch 910, loss 0.8653642535209656, mat0 norm 14.910679817199707\n",
      "Epoch 911, loss 0.8652760982513428, mat0 norm 14.909446716308594\n",
      "Epoch 912, loss 0.8651883602142334, mat0 norm 14.908215522766113\n",
      "Epoch 913, loss 0.8651005029678345, mat0 norm 14.906994819641113\n",
      "Epoch 914, loss 0.865012526512146, mat0 norm 14.905765533447266\n",
      "Epoch 915, loss 0.86492520570755, mat0 norm 14.904545783996582\n",
      "Epoch 916, loss 0.8648371696472168, mat0 norm 14.903315544128418\n",
      "Epoch 917, loss 0.8647498488426208, mat0 norm 14.902080535888672\n",
      "Epoch 918, loss 0.864662230014801, mat0 norm 14.900870323181152\n",
      "Epoch 919, loss 0.864574670791626, mat0 norm 14.899639129638672\n",
      "Epoch 920, loss 0.8644874095916748, mat0 norm 14.898420333862305\n",
      "Epoch 921, loss 0.864399790763855, mat0 norm 14.897188186645508\n",
      "Epoch 922, loss 0.8643127083778381, mat0 norm 14.895956993103027\n",
      "Epoch 923, loss 0.8642252683639526, mat0 norm 14.894745826721191\n",
      "Epoch 924, loss 0.8641380071640015, mat0 norm 14.893510818481445\n",
      "Epoch 925, loss 0.8640510439872742, mat0 norm 14.892293930053711\n",
      "Epoch 926, loss 0.8639636039733887, mat0 norm 14.891063690185547\n",
      "Epoch 927, loss 0.8638769388198853, mat0 norm 14.889835357666016\n",
      "Epoch 928, loss 0.8637897372245789, mat0 norm 14.888622283935547\n",
      "Epoch 929, loss 0.8637027740478516, mat0 norm 14.887391090393066\n",
      "Epoch 930, loss 0.8636159896850586, mat0 norm 14.886173248291016\n",
      "Epoch 931, loss 0.863528847694397, mat0 norm 14.884943962097168\n",
      "Epoch 932, loss 0.8634425401687622, mat0 norm 14.88371467590332\n",
      "Epoch 933, loss 0.8633555173873901, mat0 norm 14.882497787475586\n",
      "Epoch 934, loss 0.8632689118385315, mat0 norm 14.881271362304688\n",
      "Epoch 935, loss 0.8631823658943176, mat0 norm 14.880057334899902\n",
      "Epoch 936, loss 0.8630954623222351, mat0 norm 14.878827095031738\n",
      "Epoch 937, loss 0.8630094528198242, mat0 norm 14.877596855163574\n",
      "Epoch 938, loss 0.8629225492477417, mat0 norm 14.876384735107422\n",
      "Epoch 939, loss 0.8628363013267517, mat0 norm 14.875155448913574\n",
      "Epoch 940, loss 0.8627499341964722, mat0 norm 14.873940467834473\n",
      "Epoch 941, loss 0.8626634478569031, mat0 norm 14.872712135314941\n",
      "Epoch 942, loss 0.8625775575637817, mat0 norm 14.871504783630371\n",
      "Epoch 943, loss 0.8624911308288574, mat0 norm 14.87027359008789\n",
      "Epoch 944, loss 0.8624051809310913, mat0 norm 14.869043350219727\n",
      "Epoch 945, loss 0.8623189330101013, mat0 norm 14.867830276489258\n",
      "Epoch 946, loss 0.8622326850891113, mat0 norm 14.866605758666992\n",
      "Epoch 947, loss 0.8621470332145691, mat0 norm 14.865387916564941\n",
      "Epoch 948, loss 0.8620607852935791, mat0 norm 14.864165306091309\n",
      "Epoch 949, loss 0.8619750738143921, mat0 norm 14.862933158874512\n",
      "Epoch 950, loss 0.861889123916626, mat0 norm 14.861723899841309\n",
      "Epoch 951, loss 0.8618032932281494, mat0 norm 14.860495567321777\n",
      "Epoch 952, loss 0.8617177605628967, mat0 norm 14.85928726196289\n",
      "Epoch 953, loss 0.8616317510604858, mat0 norm 14.85805892944336\n",
      "Epoch 954, loss 0.861546516418457, mat0 norm 14.856829643249512\n",
      "Epoch 955, loss 0.8614606261253357, mat0 norm 14.855616569519043\n",
      "Epoch 956, loss 0.8613752722740173, mat0 norm 14.854389190673828\n",
      "Epoch 957, loss 0.8612897396087646, mat0 norm 14.853179931640625\n",
      "Epoch 958, loss 0.8612040877342224, mat0 norm 14.851958274841309\n",
      "Epoch 959, loss 0.8611191511154175, mat0 norm 14.850744247436523\n",
      "Epoch 960, loss 0.8610334396362305, mat0 norm 14.849516868591309\n",
      "Epoch 961, loss 0.860948383808136, mat0 norm 14.848292350769043\n",
      "Epoch 962, loss 0.8608630895614624, mat0 norm 14.84708023071289\n",
      "Epoch 963, loss 0.860777735710144, mat0 norm 14.845856666564941\n",
      "Epoch 964, loss 0.8606928586959839, mat0 norm 14.844647407531738\n",
      "Epoch 965, loss 0.8606075644493103, mat0 norm 14.84342098236084\n",
      "Epoch 966, loss 0.8605228066444397, mat0 norm 14.842195510864258\n",
      "Epoch 967, loss 0.8604376912117004, mat0 norm 14.840983390808105\n",
      "Epoch 968, loss 0.860352635383606, mat0 norm 14.839757919311523\n",
      "Epoch 969, loss 0.8602679967880249, mat0 norm 14.838553428649902\n",
      "Epoch 970, loss 0.8601827621459961, mat0 norm 14.837325096130371\n",
      "Epoch 971, loss 0.8600984811782837, mat0 norm 14.836102485656738\n",
      "Epoch 972, loss 0.860013484954834, mat0 norm 14.8348970413208\n",
      "Epoch 973, loss 0.8599288463592529, mat0 norm 14.833669662475586\n",
      "Epoch 974, loss 0.8598442077636719, mat0 norm 14.832457542419434\n",
      "Epoch 975, loss 0.859759509563446, mat0 norm 14.83123779296875\n",
      "Epoch 976, loss 0.8596752882003784, mat0 norm 14.830025672912598\n",
      "Epoch 977, loss 0.8595905303955078, mat0 norm 14.828804969787598\n",
      "Epoch 978, loss 0.8595061898231506, mat0 norm 14.827579498291016\n",
      "Epoch 979, loss 0.8594216704368591, mat0 norm 14.826371192932129\n",
      "Epoch 980, loss 0.859337329864502, mat0 norm 14.825151443481445\n",
      "Epoch 981, loss 0.8592532873153687, mat0 norm 14.823944091796875\n",
      "Epoch 982, loss 0.8591686487197876, mat0 norm 14.822717666625977\n",
      "Epoch 983, loss 0.8590859770774841, mat0 norm 14.821489334106445\n",
      "Epoch 984, loss 0.859002947807312, mat0 norm 14.820295333862305\n",
      "Epoch 985, loss 0.8589211106300354, mat0 norm 14.819145202636719\n",
      "Epoch 986, loss 0.8588398098945618, mat0 norm 14.817975997924805\n",
      "Epoch 987, loss 0.8587580323219299, mat0 norm 14.816826820373535\n",
      "Epoch 988, loss 0.8586764931678772, mat0 norm 14.815680503845215\n",
      "Epoch 989, loss 0.8585947751998901, mat0 norm 14.814528465270996\n",
      "Epoch 990, loss 0.8585135340690613, mat0 norm 14.81338119506836\n",
      "Epoch 991, loss 0.8584322333335876, mat0 norm 14.812212944030762\n",
      "Epoch 992, loss 0.8583508729934692, mat0 norm 14.811076164245605\n",
      "Epoch 993, loss 0.858269453048706, mat0 norm 14.809928894042969\n",
      "Epoch 994, loss 0.8581881523132324, mat0 norm 14.808775901794434\n",
      "Epoch 995, loss 0.8581072688102722, mat0 norm 14.807611465454102\n",
      "Epoch 996, loss 0.8580259680747986, mat0 norm 14.806468963623047\n",
      "Epoch 997, loss 0.8579446077346802, mat0 norm 14.805318832397461\n",
      "Epoch 998, loss 0.8578634262084961, mat0 norm 14.804173469543457\n",
      "Epoch 999, loss 0.8577828407287598, mat0 norm 14.803024291992188\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "\n",
    "x = torch.randn(10, 10)\n",
    "target = torch.empty(10, dtype=torch.long).random_(2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, target)\n",
    "    # new loss = loss + l1 penalization on the parameters of the first layer (dense layer)\n",
    "    l1_norm = torch.norm(model[0].weight, p=1)\n",
    "    loss += 0.01*l1_norm\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {}, loss {}, mat0 norm {}'.format(\n",
    "        epoch, loss.item(), l1_norm.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, target, eta=0.01):\n",
    "    # L1 norm + penalization for normalization of the coefficients\n",
    "    loss = F.l1_loss(output,target)+ eta* torch.norm(model[0].weight, p=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Custom layers using torch operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLinear(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ComplexLinear, self).__init__()\n",
    "        self.fc_r = Linear(in_features, out_features)\n",
    "        self.fc_i = Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self,input_r, input_i):\n",
    "        return self.fc_r(input_r)-self.fc_i(input_i), \\\n",
    "        self.fc_r(input_i)+self.fc_i(input_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Custom functions\n",
    "\n",
    "Documentation [here](https://pytorch.org/docs/stable/notes/extending.html)\n",
    "\n",
    "We can write functions from scratch.   \n",
    "It is then necessary to write the `backward()` function that calculates the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        # save current values for gradient calculation\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "        # This function has only a single output, so it gets only one gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Custom modules\n",
    "\n",
    "Documentation [here](https://pytorch.org/docs/stable/notes/extending.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can create a subclass of `torch.nn.Module` with:\n",
    "* trainable parameters by creating `nn.Parameter`s or registering parameters `register_parameter()`.\n",
    "* non trainable parameters using `register_buffer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get automatically registered as \n",
    "        # Module's parameter once it's assigned as an attribute. Parameters and buffers \n",
    "        # need to be registered, or they won't appear in .parameters() \n",
    "        # (doesn't apply to buffers), and won't be converted when e.g. .cuda() is called. \n",
    "        # You can use  .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # We call the function we created previously\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### *Just-in-time* compiling \n",
    "\n",
    "The above ways to create custom functions and layers use pure Python code.  \n",
    "Hence, the code is interpreded, on the other hand the provided building blocks are written in C++ and optimized.  \n",
    "If the code involves many calculations, it can lead to significantly long computation time.  \n",
    "\n",
    "PyTorch provide [*just-in-time compiling* tools](https://pytorch.org/docs/stable/jit.html) that will allows to write pure Python code that will be compiled and optimized automatically.\n",
    "\n",
    "It has two positive aspectes:\n",
    "\n",
    "* It allows to create models that could be used outside Python (not so much interesting for us),\n",
    "* It allows optimizations and improving performance, both during training and inference.\n",
    "\n",
    "We will very quickly go through the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 1:** Create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def foo(x, y):\n",
    "    return 2 * x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 2:** Turn the Python code into a TorchScript program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.rand(3); y = torch.rand(3)\n",
    "z = traced_foo(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " </style><div class=warn>\n",
    "**Warning:**  You need to provide examples, i.e. arbitrary tensors of the same size you will use later, for the tracing.\n",
    "Here we use `(torch.rand(3), torch.rand(3)`\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some limitations, read the [docs](https://www.google.com/search?client=ubuntu&channel=fs&q=pytorch+script+module&ie=utf-8&oe=utf-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Scripting\n",
    "\n",
    "You can write TorchScript code directly using Python syntax. You do this using:  \n",
    "* the `\\@torch.jit.script` decorator (for functions),\n",
    "* the `\\@torch.jit.script_method` decorator (for methods) on subclasses of `ScriptModule` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Scripting a function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.jit.script\n",
    "def foo(x, y):\n",
    "    if x.max() > y.max():\n",
    "        r = x\n",
    "    else:\n",
    "        r = y\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Scripting a module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "class MyModule(torch.jit.ScriptModule):\n",
    "    def __init__(self, N, M):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.rand(N, M))\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, input):\n",
    "        return self.weight.mv(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " </style><div class=warn>\n",
    "**Warning:**  The new module inherits from `torch.jit.ScriptModule` instead of `torch.nn.Module`\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some limitations, read the [docs](https://www.google.com/search?client=ubuntu&channel=fs&q=pytorch+script+module&ie=utf-8&oe=utf-8) but allows more features than tracing (see [here](https://stackoverflow.com/questions/53900396/what-are-torch-scripts-in-pytorch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Possilibty to save and load `ScriptModule`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is it?\n",
    "* Build over PyTorch\n",
    "* Simplify codes\n",
    "* Allow spending less time on *details* for first tests\n",
    "* Allow avoiding stupid mistakes (e.g. forgetting `model.eval()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Installation\n",
    "```bash\n",
    "conda install -c conda-forge pytorch-lightning \n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```bash\n",
    "pip install pytorch-lightning \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From PyTorch to Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "\n",
    "class DenseNet(Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = 28*28, out_features = 128)\n",
    "        self.fc2 = nn.Linear(in_features = 128, out_features = 256)\n",
    "        self.fc3 = nn.Linear(in_features = 256, out_features = 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch Lightning model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule\n",
    "from torch import nn\n",
    "\n",
    "class DenseNet(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = 28*28, out_features = 128)\n",
    "        self.fc2 = nn.Linear(in_features = 128, out_features = 256)\n",
    "        self.fc3 = nn.Linear(in_features = 256, out_features = 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What is the difference?**\n",
    "\n",
    "How to train the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Dataset (this part is the same for both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device  = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, \n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                             target_transform=None, \n",
    "                             download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, \n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "                             target_transform=None, \n",
    "                             download=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 350 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('>> Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "model = DenseNet().to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PyTorch Lightning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class DenseNet(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = 28*28, out_features = 128)\n",
    "        self.fc2 = nn.Linear(in_features = 128, out_features = 256)\n",
    "        self.fc3 = nn.Linear(in_features = 256, out_features = 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=1e-3, momentum=0.9)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        data, target = train_batch\n",
    "        output = self.forward(data)\n",
    "        loss = F.nll_loss(output, target, reduction='sum')\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def validation_step(self, train_batch, batch_idx):\n",
    "        data, target = train_batch\n",
    "        output = self.forward(data)\n",
    "        loss = F.nll_loss(output, target, reduction='sum')\n",
    "        self.log('val_loss', loss)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = DenseNet()\n",
    "\n",
    "##training\n",
    "# gpu\n",
    "# trainer = pl.Trainer(gpus=[0])\n",
    "# cpu\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Advantages:\n",
    "* lots of default parameters/behaviors\n",
    "* no need to take care of sending things on the device with `to.device()`\n",
    "* no need to handle `model.eval()`/`model.train()`\n",
    "* add behavior by adding functions that are called at a given time, example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`training_epoch_end()` called at the end of the epoch (after all batch are processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def training_epoch_end(self,outputs):\n",
    "            #  the function is called after every epoch is completed\n",
    "            # calculating average loss \n",
    "            avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "\n",
    "            self.log('avg_loss', avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculation on server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### `screen`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Use `screen` to have the code still running after your leave your ssh session.  \n",
    "\n",
    "`screen` is a command line window manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "During a `ssh` session:\n",
    "\n",
    "`screen` to start a screen session\n",
    "\n",
    "Everything you type during a session will live on even if you quit the ssh session or if a the connection died."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`CTRL`+`a` then `c` to create a new window  \n",
    "`CTRL`+`a` then `n` to go next window  \n",
    "`CTRL`+`a` then `p` to go next window  \n",
    "`CTRL`+`a` then `CTRL`+`a` to go the previously open window  \n",
    "`CTRL`+`a` then `k` to kill the current window  \n",
    "`CTRL`+`a` then `|` to split vertically\n",
    "`CTRL`+`a` then `TAB` to navigate through split subscreens  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "to resume after closing a session:\n",
    "`screen -r`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "if there is more than one session open, you need the id of the session\n",
    "`screen -list`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "There are screens on:\n",
    "        28840.pts-10.smp-laptop (29/06/2019 20:36:01)   (Attached)\n",
    "        28391.pts-6.smp-laptop  (29/06/2019 20:33:15)   (Detached)\n",
    "        14830.pts-9.smp-laptop  (28/06/2019 18:25:30)   (Detached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`screen -r 28391`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jupyter\n",
    "\n",
    "#### Connect to  fermat\n",
    "\n",
    "```bash\n",
    "ssh login@fermat\n",
    "```\n",
    "\n",
    " </style><div class=warn>\n",
    "**Note:** \n",
    "* Create an RSA key not having to input the password evrytime,\n",
    "* If your are outise the lab, you have to go through the gateway first (bastion).\n",
    "</div>\n",
    "\n",
    "More information on the wiki!\n",
    "[https://wiki.institut-langevin.espci.fr/index.php/Serveurs_de_calcul_mutualisÃ©s](https://wiki.institut-langevin.espci.fr/index.php/Serveurs_de_calcul_mutualis%C3%A9s) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Activate the Python environment \n",
    "\n",
    "```bash\n",
    "source activate\n",
    "conda activate py38\n",
    "```\n",
    "\n",
    "Launch Jupyter\n",
    "```bash\n",
    "jupyter_il notebook\n",
    "```\n",
    "\n",
    "* Copy the adress the generated, something like:\n",
    "\n",
    "```bash\n",
    "http://localhost:8888/lab?token=5a3adc5440023eb0b115647068708a12171c3d95146316d66\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Create an SSH tunnel\n",
    "\n",
    "```bash\n",
    "ssh -L localhost:8888:localhost:8888 login@fermat\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Paste the adress in your browser.\n",
    "\n",
    "Possible to make a script that does everyhing in one shot, contact me if you need to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Neptune.ai](https://neptune.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): NVML Shared Library Not Found. GPU usage metrics may not be reported. For more information, see https://docs.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/shared/pytorch-lightning-integration/e/PYTOR-161599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeptuneLogger will work in online mode\n",
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 100 K \n",
      "1 | fc2  | Linear | 33.0 K\n",
      "2 | fc3  | Linear | 2.6 K \n",
      "--------------------------------\n",
      "136 K     Trainable params\n",
      "0         Non-trainable params\n",
      "136 K     Total params\n",
      "/opt/miniconda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/opt/miniconda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19b9fe160ff4a05881f304f47a5fd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df293bc6617c40df8cf0116815499731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "neptune_logger = pl_loggers.neptune.NeptuneLogger(\n",
    "    api_key=\"ANONYMOUS\",\n",
    "    project_name=\"shared/pytorch-lightning-integration\",\n",
    "    params={'lr':learning_rate, 'batch_size':batch_size} # save relevant paramters\n",
    "    )\n",
    "\n",
    "model = DenseNet()\n",
    "\n",
    "trainer = pl.Trainer(logger = neptune_logger)\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### TensorBoard\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard/) is a vizualization software developped for TensorFlow.  \n",
    "TensorBoard provides a suite of web applications that help us to inspect and display results in a wep page.\n",
    "\n",
    "* TensorBoard documentation [here](https://www.tensorflow.org/tensorboard/)  \n",
    "* `torch.util.tensorboard` documentation [here](https://pytorch.org/docs/stable/tensorboard.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![](img/tensorboard.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To install on the local machine, i.e. your computer (already install on server)\n",
    "\n",
    "`conda install tensorboard`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "x = torch.arange(10)\n",
    "y = x**2\n",
    "\n",
    "myfigure = plt.figure()\n",
    "plt.plot(x.numpy(),y.numpy()) \n",
    "writer.add_figure(\"matplotlib/figure\", myfigure)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "model = torchvision.models.resnet50(False)\n",
    "# Have ResNet model take in grayscale rather than RGB\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "writer.add_image('My images', grid, 0)\n",
    "writer.add_graph(model, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Launch tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Command line**\n",
    "\n",
    "`tensorboard --logdir=runs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Jupyter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f3f8bafbd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Running on server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Forward the port \n",
    "\n",
    "(on the local machine)\n",
    "`ssh -N -f -L localhost:16006:localhost:6006 <user>@fermat`          \n",
    "\n",
    " </style><div class=warn>\n",
    "**Warning:**  If 6006 used, change to something else.\n",
    "<div/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(on the server)\n",
    "`tensorboard --logdir <path> --port 6006`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(on the local machine)\n",
    "\n",
    "go to http://127.0.0.1:16006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ceap solution: `asciiplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`pip install asciiplotlib`  \n",
    "already installed on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  90 +----------------------------------------------------------------------+\n",
      "     |                                                                      |\n",
      "  80 |                                                                     *|\n",
      "     |                                                                   ** |\n",
      "     |                                                                 **   |\n",
      "  70 |                                                               **     |\n",
      "     |                                                             **       |\n",
      "  60 |                                                           **         |\n",
      "     |                                                         **           |\n",
      "  50 |                                                       **             |\n",
      "     |                                                     **               |\n",
      "     |                                                  ***                 |\n",
      "  40 |                                                **                    |\n",
      "     |                                            ****                      |\n",
      "  30 |                                        ****                          |\n",
      "     |                                     ***                              |\n",
      "  20 |                                 ****                                 |\n",
      "     |                             ****                                     |\n",
      "     |                         ****                                         |\n",
      "  10 |                   ******                                             |\n",
      "     |           ********                                                   |\n",
      "   0 +----------------------------------------------------------------------+\n",
      "     0       1       2       3       4      5       6       7       8       9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import asciiplotlib as apl\n",
    "\n",
    "x = torch.arange(10)\n",
    "y = x**2\n",
    "\n",
    "fig = apl.figure()\n",
    "fig.plot(x.numpy(),y.numpy(), width=80, height=25)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources:\n",
    "\n",
    "**General tutorials**\n",
    "* https://www.youtube.com/playlist?list=PLlMkM4tgfjnJ3I-dbhO9JTw7gNty6o_2m&disable_polymer=true\n",
    "* https://cise.ufl.edu/~xiaoyong/materials/pytorch_tutorial.pdf\n",
    "* https://www.tutorialspoint.com/pytorch/pytorch_tutorial.pdf\n",
    "\n",
    "**PyTorch examples**\n",
    "* https://github.com/pytorch/examples/\n",
    "\n",
    "**Variables and Autograd**\n",
    "* https://medium.com/@zhang_yang/gradient-calculation-examples-in-pytorch-bffe287d8634\n",
    "* https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
    "\n",
    "**Convolutional layers visualizations**\n",
    "* https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "\n",
    "**Cross entropy**\n",
    "* https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "\n",
    "**Optimizers**\n",
    "* http://ruder.io/optimizing-gradient-descent/\n",
    "* https://blog.algorithmia.com/introduction-to-optimizers/\n",
    "* https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1\n",
    "* https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook FormationPyTorch.ipynb to html\n",
      "[NbConvertApp] Writing 642260 bytes to FormationPyTorch.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html FormationPyTorch.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "160px",
    "width": "216px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "990px",
    "left": "55px",
    "top": "111.417px",
    "width": "469px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
